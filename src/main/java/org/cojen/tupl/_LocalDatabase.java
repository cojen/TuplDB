/*
 *  Copyright (C) 2011-2017 Cojen.org
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Affero General Public License as
 *  published by the Free Software Foundation, either version 3 of the
 *  License, or (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Affero General Public License for more details.
 *
 *  You should have received a copy of the GNU Affero General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

package org.cojen.tupl;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.BufferedWriter;
import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.InputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;

import java.lang.ref.Reference;
import java.lang.ref.ReferenceQueue;
import java.lang.ref.SoftReference;

import java.math.BigInteger;

import java.nio.charset.StandardCharsets;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.EnumSet;
import java.util.Map;
import java.util.Set;

import java.util.concurrent.ConcurrentSkipListMap;
import java.util.concurrent.Executor;
import java.util.concurrent.Executors;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadLocalRandom;
import java.util.concurrent.TimeUnit;

import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
import java.util.concurrent.atomic.AtomicLongFieldUpdater;

import java.util.concurrent.locks.ReentrantLock;

import static java.lang.System.arraycopy;

import static java.util.Arrays.fill;

import org.cojen.tupl.ext.RecoveryHandler;
import org.cojen.tupl.ext.ReplicationManager;
import org.cojen.tupl.ext.TransactionHandler;

import org.cojen.tupl.io.FileFactory;
import org.cojen.tupl.io.MappedPageArray;
import org.cojen.tupl.io.OpenOption;
import org.cojen.tupl.io.PageArray;

import org.cojen.tupl.util.Latch;

import static org.cojen.tupl._Node.*;
import static org.cojen.tupl.DirectPageOps.*;
import static org.cojen.tupl.Utils.*;

/**
 * Standard database implementation.
 *
 * @author Generated by PageAccessTransformer from LocalDatabase.java
 */
final class _LocalDatabase extends AbstractDatabase {
    private static final int DEFAULT_CACHED_NODES = 1000;
    // +2 for registry and key map root nodes, +1 for one user index, and +2 for at least one
    // usage list to function correctly.
    private static final int MIN_CACHED_NODES = 5;

    private static final long PRIMER_MAGIC_NUMBER = 4943712973215968399L;

    private static final String INFO_FILE_SUFFIX = ".info";
    private static final String LOCK_FILE_SUFFIX = ".lock";
    static final String PRIMER_FILE_SUFFIX = ".primer";
    static final String REDO_FILE_SUFFIX = ".redo.";

    private static int nodeCountFromBytes(long bytes, int pageSize) {
        if (bytes <= 0) {
            return 0;
        }
        pageSize += NODE_OVERHEAD;
        bytes += pageSize - 1;
        if (bytes <= 0) {
            // Overflow.
            return Integer.MAX_VALUE;
        }
        long count = bytes / pageSize;
        return count <= Integer.MAX_VALUE ? (int) count : Integer.MAX_VALUE;
    }

    private static long byteCountFromNodes(int nodes, int pageSize) {
        return nodes * (long) (pageSize + NODE_OVERHEAD);
    }

    private static final int ENCODING_VERSION = 20130112;

    private static final int I_ENCODING_VERSION        = 0;
    private static final int I_ROOT_PAGE_ID            = I_ENCODING_VERSION + 4;
    private static final int I_MASTER_UNDO_LOG_PAGE_ID = I_ROOT_PAGE_ID + 8;
    private static final int I_TRANSACTION_ID          = I_MASTER_UNDO_LOG_PAGE_ID + 8;
    private static final int I_CHECKPOINT_NUMBER       = I_TRANSACTION_ID + 8;
    private static final int I_REDO_TXN_ID             = I_CHECKPOINT_NUMBER + 8;
    private static final int I_REDO_POSITION           = I_REDO_TXN_ID + 8;
    private static final int I_REPL_ENCODING           = I_REDO_POSITION + 8;
    private static final int HEADER_SIZE               = I_REPL_ENCODING + 8;

    private static final int DEFAULT_PAGE_SIZE = 4096;
    private static final int MINIMUM_PAGE_SIZE = 512;
    private static final int MAXIMUM_PAGE_SIZE = 65536;

    private static final int OPEN_REGULAR = 0, OPEN_DESTROY = 1, OPEN_TEMP = 2;

    final EventListener mEventListener;

    final TransactionHandler mCustomTxnHandler;

    final RecoveryHandler mRecoveryHandler;
    private LHashTable.Obj<_LocalTransaction> mRecoveredTransactions;

    private final File mBaseFile;
    private final boolean mReadOnly;
    private final LockedFile mLockFile;

    final DurabilityMode mDurabilityMode;
    final long mDefaultLockTimeoutNanos;
    final _LockManager mLockManager;
    private final ThreadLocal<SoftReference<_LocalTransaction>> mLocalTransaction;
    final _RedoWriter mRedoWriter;
    final _PageDb mPageDb;
    final int mPageSize;

    private final _PagePool mSparePagePool;

    private final Object mArena;
    private final _NodeContext[] mNodeContexts;

    private final CommitLock mCommitLock;

    // Is either CACHED_DIRTY_0 or CACHED_DIRTY_1. Access is guarded by commit lock.
    private byte mCommitState;

    // State to apply to nodes which have just been read. Is CACHED_DIRTY_0 for empty databases
    // which have never checkpointed, but is CACHED_CLEAN otherwise.
    private volatile byte mInitialReadState = CACHED_CLEAN;

    // Set during checkpoint after commit state has switched. If checkpoint aborts, next
    // checkpoint will resume with this commit header and master undo log.
    /*P*/ // [
    // private byte[] mCommitHeader;
    /*P*/ // |
    private volatile long mCommitHeader = p_null();
    private static final AtomicLongFieldUpdater<_LocalDatabase> cCommitHeaderUpdater =
        AtomicLongFieldUpdater.newUpdater(_LocalDatabase.class, "mCommitHeader");
    /*P*/ // ]
    private _UndoLog mCommitMasterUndoLog;

    // Typically opposite of mCommitState, or negative if checkpoint is not in
    // progress. Indicates which nodes are being flushed by the checkpoint.
    private volatile int mCheckpointFlushState = CHECKPOINT_NOT_FLUSHING;

    private static final int CHECKPOINT_FLUSH_PREPARE = -2, CHECKPOINT_NOT_FLUSHING = -1;

    // The root tree, which maps tree ids to other tree root node ids.
    private final _Tree mRegistry;

    static final byte KEY_TYPE_INDEX_NAME   = 0; // prefix for name to id mapping
    static final byte KEY_TYPE_INDEX_ID     = 1; // prefix for id to name mapping
    static final byte KEY_TYPE_TREE_ID_MASK = 2; // full key for random tree id mask
    static final byte KEY_TYPE_NEXT_TREE_ID = 3; // full key for tree id sequence
    static final byte KEY_TYPE_TRASH_ID     = 4; // prefix for id to name mapping of trash

    // Various mappings, defined by KEY_TYPE_ fields.
    private final _Tree mRegistryKeyMap;

    private final Latch mOpenTreesLatch;
    // Maps tree names to open trees.
    // Must be a concurrent map because we rely on concurrent iteration.
    private final Map<byte[], _TreeRef> mOpenTrees;
    private final LHashTable.Obj<_TreeRef> mOpenTreesById;
    private final ReferenceQueue<_Tree> mOpenTreesRefQueue;

    // Map of all loaded nodes.
    private final _Node[] mNodeMapTable;
    private final Latch[] mNodeMapLatches;

    final int mMaxKeySize;
    final int mMaxEntrySize;
    final int mMaxFragmentedEntrySize;

    // Fragmented values which are transactionally deleted go here.
    private volatile _FragmentedTrash mFragmentedTrash;

    // Pre-calculated maximum capacities for inode levels.
    private final long[] mFragmentInodeLevelCaps;

    // Stripe the transaction contexts, for improved concurrency.
    private final _TransactionContext[] mTxnContexts;

    // Checkpoint lock is fair, to ensure that user checkpoint requests are not stalled for too
    // long by checkpoint thread.
    private final ReentrantLock mCheckpointLock = new ReentrantLock(true);

    private long mLastCheckpointNanos;

    private volatile Checkpointer mCheckpointer;

    final TempFileManager mTempFileManager;

    /*P*/ // [|
    final boolean mFullyMapped;
    /*P*/ // ]

    private volatile ExecutorService mSorterExecutor;

    // Maps registered cursor ids to index ids.
    private _Tree mCursorRegistry;

    private volatile int mClosed;
    private volatile Throwable mClosedCause;

    private static final AtomicIntegerFieldUpdater<_LocalDatabase>
        cClosedUpdater = AtomicIntegerFieldUpdater.newUpdater(_LocalDatabase.class, "mClosed");

    /**
     * Open a database, creating it if necessary.
     */
    static _LocalDatabase open(DatabaseConfig config) throws IOException {
        config = config.clone();
        _LocalDatabase db = new _LocalDatabase(config, OPEN_REGULAR);
        try {
            db.finishInit(config);
            return db;
        } catch (Throwable e) {
            closeQuietly(db);
            throw e;
        }
    }

    /**
     * Delete the contents of an existing database, and replace it with an
     * empty one. When using a raw block device for the data file, this method
     * must be used to format it.
     */
    static _LocalDatabase destroy(DatabaseConfig config) throws IOException {
        config = config.clone();
        if (config.mReadOnly) {
            throw new IllegalArgumentException("Cannot destroy read-only database");
        }
        _LocalDatabase db = new _LocalDatabase(config, OPEN_DESTROY);
        try {
            db.finishInit(config);
            return db;
        } catch (Throwable e) {
            closeQuietly(db);
            throw e;
        }
    }

    /**
     * @param config base file is set as a side-effect
     */
    static _Tree openTemp(TempFileManager tfm, DatabaseConfig config) throws IOException {
        File file = tfm.createTempFile();
        config.baseFile(file);
        config.dataFile(file);
        config.createFilePath(false);
        config.durabilityMode(DurabilityMode.NO_FLUSH);
        _LocalDatabase db = new _LocalDatabase(config, OPEN_TEMP);
        tfm.register(file, db);
        db.mCheckpointer = new Checkpointer(db, config);
        db.mCheckpointer.start(false);
        return db.mRegistry;
    }

    /**
     * @param config unshared config
     */
    private _LocalDatabase(DatabaseConfig config, int openMode) throws IOException {
        config.mEventListener = mEventListener = SafeEventListener.makeSafe(config.mEventListener);

        mCustomTxnHandler = config.mTxnHandler;
        mRecoveryHandler = config.mRecoveryHandler;

        mBaseFile = config.mBaseFile;
        mReadOnly = config.mReadOnly;
        final File[] dataFiles = config.dataFiles();

        int pageSize = config.mPageSize;
        boolean explicitPageSize = true;
        if (pageSize <= 0) {
            config.pageSize(pageSize = DEFAULT_PAGE_SIZE);
            explicitPageSize = false;
        } else if (pageSize < MINIMUM_PAGE_SIZE) {
            throw new IllegalArgumentException
                ("Page size is too small: " + pageSize + " < " + MINIMUM_PAGE_SIZE);
        } else if (pageSize > MAXIMUM_PAGE_SIZE) {
            throw new IllegalArgumentException
                ("Page size is too large: " + pageSize + " > " + MAXIMUM_PAGE_SIZE);
        } else if ((pageSize & 1) != 0) {
            throw new IllegalArgumentException
                ("Page size must be even: " + pageSize);
        }

        int minCache, maxCache;
        cacheSize: {
            long minCachedBytes = Math.max(0, config.mMinCachedBytes);
            long maxCachedBytes = Math.max(0, config.mMaxCachedBytes);

            if (maxCachedBytes == 0) {
                maxCachedBytes = minCachedBytes;
                if (maxCachedBytes == 0) {
                    minCache = maxCache = DEFAULT_CACHED_NODES;
                    break cacheSize;
                }
            }

            if (minCachedBytes > maxCachedBytes) {
                throw new IllegalArgumentException
                    ("Minimum cache size exceeds maximum: " +
                     minCachedBytes + " > " + maxCachedBytes);
            }

            minCache = nodeCountFromBytes(minCachedBytes, pageSize);
            maxCache = nodeCountFromBytes(maxCachedBytes, pageSize);

            minCache = Math.max(MIN_CACHED_NODES, minCache);
            maxCache = Math.max(MIN_CACHED_NODES, maxCache);
        }

        // Update config such that info file is correct.
        config.mMinCachedBytes = byteCountFromNodes(minCache, pageSize);
        config.mMaxCachedBytes = byteCountFromNodes(maxCache, pageSize);

        mDurabilityMode = config.mDurabilityMode;
        mDefaultLockTimeoutNanos = config.mLockTimeoutNanos;
        mLockManager = new _LockManager(this, config.mLockUpgradeRule, mDefaultLockTimeoutNanos);
        mLocalTransaction = new ThreadLocal<>();

        // Initialize NodeMap, the primary cache of Nodes.
        final int procCount = Runtime.getRuntime().availableProcessors();
        {
            int capacity = Utils.roundUpPower2(maxCache);
            if (capacity < 0) {
                capacity = 0x40000000;
            }
            // The number of latches must not be more than the number of hash buckets. This
            // ensures that a hash bucket is guarded by exactly one latch, which can be shared
            // across multiple buckets.
            int latches = Math.min(capacity, Utils.roundUpPower2(procCount * 16));
            mNodeMapTable = new _Node[capacity];
            mNodeMapLatches = new Latch[latches];
            for (int i=0; i<latches; i++) {
                mNodeMapLatches[i] = new Latch();
            }
        }

        if (mBaseFile != null && !mReadOnly && config.mMkdirs) {
            FileFactory factory = config.mFileFactory;

            final boolean baseDirectoriesCreated;
            File baseDir = mBaseFile.getParentFile();
            if (factory == null) {
                baseDirectoriesCreated = baseDir.mkdirs();
            } else {
                baseDirectoriesCreated = factory.createDirectories(baseDir);
            }

            if (!baseDirectoriesCreated && !baseDir.exists()) {
                throw new FileNotFoundException("Could not create directory: " + baseDir);
            }

            if (dataFiles != null) {
                for (File f : dataFiles) {
                    final boolean dataDirectoriesCreated;
                    File dataDir = f.getParentFile();
                    if (factory == null) {
                        dataDirectoriesCreated = dataDir.mkdirs();
                    } else {
                        dataDirectoriesCreated = factory.createDirectories(dataDir);
                    }

                    if (!dataDirectoriesCreated && !dataDir.exists()) {
                        throw new FileNotFoundException("Could not create directory: " + dataDir);
                    }
                }
            }
        }

        try {
            // Create lock file, preventing database from being opened multiple times.
            if (mBaseFile == null || openMode == OPEN_TEMP) {
                mLockFile = null;
            } else {
                File lockFile = new File(mBaseFile.getPath() + LOCK_FILE_SUFFIX);

                FileFactory factory = config.mFileFactory;
                if (factory != null && !mReadOnly) {
                    factory.createFile(lockFile);
                }

                mLockFile = new LockedFile(lockFile, mReadOnly);
            }

            if (openMode == OPEN_DESTROY) {
                deleteRedoLogFiles();
            }

            final long cacheInitStart = System.nanoTime();

            // Create or retrieve optional page cache.
            PageCache cache = config.pageCache(mEventListener);

            if (cache != null) {
                // Update config such that info file is correct.
                config.mSecondaryCacheSize = cache.capacity();
            }

            /*P*/ // [|
            boolean fullyMapped = false;
            /*P*/ // ]

            EventListener debugListener = null;
            if (config.mDebugOpen != null) {
                debugListener = mEventListener;
            }

            if (dataFiles == null) {
                PageArray dataPageArray = config.mDataPageArray;
                if (dataPageArray == null) {
                    mPageDb = new _NonPageDb(pageSize, cache);
                } else {
                    dataPageArray = dataPageArray.open();
                    Crypto crypto = config.mCrypto;
                    mPageDb = _DurablePageDb.open
                        (debugListener, dataPageArray, cache, crypto, openMode == OPEN_DESTROY);
                    /*P*/ // [|
                    fullyMapped = crypto == null && cache == null
                                  && dataPageArray instanceof MappedPageArray;
                    /*P*/ // ]
                }
            } else {
                EnumSet<OpenOption> options = config.createOpenOptions();

                _PageDb pageDb;
                try {
                    pageDb = _DurablePageDb.open
                        (debugListener, explicitPageSize, pageSize,
                         dataFiles, config.mFileFactory, options,
                         cache, config.mCrypto, openMode == OPEN_DESTROY);
                } catch (FileNotFoundException e) {
                    if (!mReadOnly) {
                        throw e;
                    }
                    pageDb = new _NonPageDb(pageSize, cache);
                }

                mPageDb = pageDb;
            }

            /*P*/ // [|
            mFullyMapped = fullyMapped;
            /*P*/ // ]

            // Actual page size might differ from configured size.
            config.pageSize(pageSize = mPageSize = mPageDb.pageSize());

            /*P*/ // [
            // config.mDirectPageAccess = false;
            /*P*/ // |
            config.mDirectPageAccess = true;
            /*P*/ // ]

            // Write info file of properties, after database has been opened and after page
            // size is truly known.
            if (mBaseFile != null && openMode != OPEN_TEMP && !mReadOnly) {
                File infoFile = new File(mBaseFile.getPath() + INFO_FILE_SUFFIX);

                FileFactory factory = config.mFileFactory;
                if (factory != null) {
                    factory.createFile(infoFile);
                }

                BufferedWriter w = new BufferedWriter
                    (new OutputStreamWriter(new FileOutputStream(infoFile),
                                            StandardCharsets.UTF_8));

                try {
                    config.writeInfo(w);
                } finally {
                    w.close();
                }
            }

            mCommitLock = mPageDb.commitLock();

            // Pre-allocate nodes. They are automatically added to the node context usage
            // lists, and so nothing special needs to be done to allow them to get used. Since
            // the initial state is clean, evicting these nodes does nothing.

            if (mEventListener != null) {
                mEventListener.notify(EventType.CACHE_INIT_BEGIN,
                                      "Initializing %1$d cached nodes", minCache);
            }

            _NodeContext[] contexts;
            try {
                // Try to allocate the minimum cache size into an arena, which has lower memory
                // overhead, is page aligned, and takes less time to zero-fill.
                arenaAlloc: {
                    // If database is fully mapped, then no cached pages are allocated at all.
                    // Nodes point directly to a mapped region of memory.
                    /*P*/ // [|
                    if (mFullyMapped) {
                        mArena = null;
                        break arenaAlloc;
                    }
                    /*P*/ // ]

                    try {
                        mArena = p_arenaAlloc(pageSize, minCache); 
                    } catch (IOException e) {
                        OutOfMemoryError oom = new OutOfMemoryError();
                        oom.initCause(e);
                        throw oom;
                    }
                }

                // Magic constant was determined empirically against the G1 collector. A higher
                // constant increases memory thrashing.
                long usedRate = Utils.roundUpPower2((long) Math.ceil(maxCache / 32768)) - 1;

                int stripes = roundUpPower2(procCount * 4);

                int stripeSize;
                while (true) {
                    stripeSize = maxCache / stripes;
                    if (stripes <= 1 || stripeSize >= 100) {
                        break;
                    }
                    stripes >>= 1;
                }

                int rem = maxCache % stripes;

                contexts = new _NodeContext[stripes];

                for (int i=0; i<stripes; i++) {
                    int size = stripeSize;
                    if (rem > 0) {
                        size++;
                        rem--;
                    }
                    contexts[i] = new _NodeContext(this, usedRate, size);
                }

                stripeSize = minCache / stripes;
                rem = minCache % stripes;

                for (_NodeContext context : contexts) {
                    int size = stripeSize;
                    if (rem > 0) {
                        size++;
                        rem--;
                    }
                    context.initialize(mArena, size);
                }
            } catch (OutOfMemoryError e) {
                contexts = null;
                OutOfMemoryError oom = new OutOfMemoryError
                    ("Unable to allocate the minimum required number of cached nodes: " +
                     minCache + " (" + (minCache * (long) (pageSize + NODE_OVERHEAD)) + " bytes)");
                oom.initCause(e.getCause());
                throw oom;
            }

            mNodeContexts = contexts;

            if (mEventListener != null) {
                double duration = (System.nanoTime() - cacheInitStart) / 1_000_000_000.0;
                mEventListener.notify(EventType.CACHE_INIT_COMPLETE,
                                      "Cache initialization completed in %1$1.3f seconds",
                                      duration, TimeUnit.SECONDS);
            }

            mTxnContexts = new _TransactionContext[procCount * 4];
            for (int i=0; i<mTxnContexts.length; i++) {
                mTxnContexts[i] = new _TransactionContext(mTxnContexts.length, 4096);
            };

            mSparePagePool = new _PagePool(mPageSize, procCount, mPageDb.isDirectIO());

            mCommitLock.acquireExclusive();
            try {
                mCommitState = CACHED_DIRTY_0;
            } finally {
                mCommitLock.releaseExclusive();
            }

            byte[] header = new byte[HEADER_SIZE];
            mPageDb.readExtraCommitData(header);

            // Also verifies the database and replication encodings.
            _Node rootNode = loadRegistryRoot(config, header);

            // Cannot call newTreeInstance because mRedoWriter isn't set yet.
            if (config.mReplManager != null) {
                mRegistry = new _TxnTree(this, _Tree.REGISTRY_ID, null, rootNode);
            } else {
                mRegistry = new _Tree(this, _Tree.REGISTRY_ID, null, rootNode);
            }

            mOpenTreesLatch = new Latch();
            if (openMode == OPEN_TEMP) {
                mOpenTrees = Collections.emptyMap();
                mOpenTreesById = new LHashTable.Obj<>(0);
                mOpenTreesRefQueue = null;
            } else {
                mOpenTrees = new ConcurrentSkipListMap<>(KeyComparator.THE);
                mOpenTreesById = new LHashTable.Obj<>(16);
                mOpenTreesRefQueue = new ReferenceQueue<>();
            }

            long txnId = decodeLongLE(header, I_TRANSACTION_ID);
            if (txnId < 0) {
                throw new CorruptDatabaseException("Invalid transaction id: " + txnId);
            }

            long redoNum = decodeLongLE(header, I_CHECKPOINT_NUMBER);
            long redoPos = decodeLongLE(header, I_REDO_POSITION);
            long redoTxnId = decodeLongLE(header, I_REDO_TXN_ID);

            if (debugListener != null) {
                debugListener.notify(EventType.DEBUG, "MASTER_UNDO_LOG_PAGE_ID: %1$d",
                                     decodeLongLE(header, I_MASTER_UNDO_LOG_PAGE_ID));
                debugListener.notify(EventType.DEBUG, "TRANSACTION_ID: %1$d", txnId);
                debugListener.notify(EventType.DEBUG, "CHECKPOINT_NUMBER: %1$d", redoNum);
                debugListener.notify(EventType.DEBUG, "REDO_TXN_ID: %1$d", redoTxnId);
                debugListener.notify(EventType.DEBUG, "REDO_POSITION: %1$d", redoPos);
            }

            if (openMode == OPEN_TEMP) {
                mRegistryKeyMap = null;
            } else {
                mRegistryKeyMap = openInternalTree(_Tree.REGISTRY_KEY_MAP_ID, true, config);
                if (debugListener != null) {
                    Cursor c = indexRegistryById().newCursor(Transaction.BOGUS);
                    try {
                        for (c.first(); c.key() != null; c.next()) {
                            long indexId = decodeLongBE(c.key(), 0);
                            String nameStr = new String(c.value(), StandardCharsets.UTF_8);
                            debugListener.notify(EventType.DEBUG, "Index: id=%1$d, name=%2$s",
                                                 indexId, nameStr);
                        }
                    } finally {
                        c.reset();
                    }
                }
            }

            _Tree cursorRegistry = null;
            if (openMode != OPEN_TEMP) {
                _Tree tree = openInternalTree(_Tree.FRAGMENTED_TRASH_ID, false, config);
                if (tree != null) {
                    mFragmentedTrash = new _FragmentedTrash(tree);
                }
                cursorRegistry = openInternalTree(_Tree.CURSOR_REGISTRY_ID, false, config);
            }

            // Limit maximum non-fragmented entry size to 0.75 of usable node size.
            mMaxEntrySize = ((pageSize - _Node.TN_HEADER_SIZE) * 3) >> 2;

            // Limit maximum fragmented entry size to guarantee that 2 entries fit. Each also
            // requires 2 bytes for pointer and up to 3 bytes for value length field.
            mMaxFragmentedEntrySize = (pageSize - _Node.TN_HEADER_SIZE - (2 + 3 + 2 + 3)) >> 1;

            // Limit the maximum key size to allow enough room for a fragmented value. It might
            // require up to 11 bytes for fragment encoding (when length is >= 65536), and
            // additional bytes are required for the value header inside the tree node.
            mMaxKeySize = Math.min(16383, mMaxFragmentedEntrySize - (2 + 11));

            mFragmentInodeLevelCaps = calculateInodeLevelCaps(mPageSize);

            long recoveryStart = 0;
            if (mBaseFile == null || openMode == OPEN_TEMP) {
                mRedoWriter = null;
            } else {
                // Perform recovery by examining redo and undo logs.

                if (mEventListener != null) {
                    mEventListener.notify(EventType.RECOVERY_BEGIN, "Database recovery begin");
                    recoveryStart = System.nanoTime();
                }

                LHashTable.Obj<_LocalTransaction> txns = new LHashTable.Obj<>(16);
                {
                    long masterNodeId = decodeLongLE(header, I_MASTER_UNDO_LOG_PAGE_ID);
                    if (masterNodeId != 0) {
                        if (mEventListener != null) {
                            mEventListener.notify
                                (EventType.RECOVERY_LOAD_UNDO_LOGS, "Loading undo logs");
                        }

                        _UndoLog master = _UndoLog.recoverMasterUndoLog(this, masterNodeId);

                        boolean trace = debugListener != null &&
                            Boolean.TRUE.equals(config.mDebugOpen.get("traceUndo"));

                        master.recoverTransactions
                            (debugListener, trace, txns, LockMode.UPGRADABLE_READ, 0);
                    }
                }

                LHashTable.Obj<_TreeCursor> cursors = new LHashTable.Obj<>(4);
                if (cursorRegistry != null) {
                    Cursor c = cursorRegistry.newCursor(Transaction.BOGUS);
                    for (c.first(); c.key() != null; c.next()) {
                        long cursorId = decodeLongBE(c.key(), 0);
                        long indexId = decodeLongBE(c.value(), 0);
                        _Tree tree = (_Tree) anyIndexById(indexId);
                        _TreeCursor cursor = new _TreeCursor(tree);
                        cursor.mKeyOnly = true;
                        cursor.mCursorId = cursorId;
                        cursors.insert(cursorId).value = cursor;
                    }
                    cursorRegistry.forceClose();
                }

                if (mCustomTxnHandler != null) {
                    // Although handler shouldn't access the database yet, be safe and call
                    // this method at the point that the database is mostly functional. All
                    // other custom methods will be called soon as well.
                    mCustomTxnHandler.setCheckpointLock(this, mCommitLock);
                }

                ReplicationManager rm = config.mReplManager;
                if (rm != null) {
                    if (mEventListener != null) {
                        mEventListener.notify(EventType.REPLICATION_DEBUG,
                                              "Starting at: %1$d", redoPos);
                    }

                    rm.start(redoPos);

                    if (mReadOnly) {
                        mRedoWriter = null;

                        if (debugListener != null &&
                            Boolean.TRUE.equals(config.mDebugOpen.get("traceRedo")))
                        {
                            RedoEventPrinter printer = new RedoEventPrinter
                                (debugListener, EventType.DEBUG);
                            new ReplRedoDecoder(rm, redoPos, redoTxnId, new Latch()).run(printer);
                        }
                    } else {
                        _ReplRedoEngine engine = new _ReplRedoEngine
                            (rm, config.mMaxReplicaThreads, this, txns, cursors);
                        mRedoWriter = engine.initWriter(redoNum);

                        // Cannot start recovery until constructor is finished and final field
                        // values are visible to other threads. Pass the state to the caller
                        // through the config object.
                        config.mReplRecoveryStartNanos = recoveryStart;
                        config.mReplInitialTxnId = redoTxnId;
                    }
                } else {
                    // Apply cache primer before applying redo logs.
                    applyCachePrimer(config);

                    final long logId = redoNum;

                    if (mReadOnly) {
                        mRedoWriter = null;

                        if (debugListener != null &&
                            Boolean.TRUE.equals(config.mDebugOpen.get("traceRedo")))
                        {
                            RedoEventPrinter printer = new RedoEventPrinter
                                (debugListener, EventType.DEBUG);

                            _RedoLog replayLog = new _RedoLog(config, logId, redoPos);

                            replayLog.replay
                                (printer, debugListener, EventType.RECOVERY_APPLY_REDO_LOG,
                                 "Applying redo log: %1$d");
                        }
                    } else {
                        // Make sure old redo logs are deleted. Process might have exited
                        // before last checkpoint could delete them.
                        for (int i=1; i<=2; i++) {
                            _RedoLog.deleteOldFile(config.mBaseFile, logId - i);
                        }

                        _RedoLogApplier applier = new _RedoLogApplier(this, txns, cursors);
                        _RedoLog replayLog = new _RedoLog(config, logId, redoPos);

                        // As a side-effect, log id is set one higher than last file scanned.
                        Set<File> redoFiles = replayLog.replay
                            (applier, mEventListener, EventType.RECOVERY_APPLY_REDO_LOG,
                             "Applying redo log: %1$d");

                        boolean doCheckpoint = !redoFiles.isEmpty();

                        // Avoid re-using transaction ids used by recovery.
                        redoTxnId = applier.mHighestTxnId;
                        if (redoTxnId != 0) {
                            // Subtract for modulo comparison.
                            if (txnId == 0 || (redoTxnId - txnId) > 0) {
                                txnId = redoTxnId;
                            }
                        }

                        if (txns.size() > 0) {
                            // Rollback or truncate all remaining transactions. They were never
                            // explicitly rolled back, or they were committed but not cleaned up.

                            if (mEventListener != null) {
                                mEventListener.notify
                                    (EventType.RECOVERY_PROCESS_REMAINING,
                                     "Processing remaining transactions");
                            }

                            txns.traverse(entry -> {
                                return entry.value.recoveryCleanup(true);
                            });

                            if (shouldInvokeRecoveryHandler(txns)) {
                                // Invoke the handler later, when database is fully opened.
                                mRecoveredTransactions = txns;
                            }

                            doCheckpoint = true;
                        }

                        // Reset any lingering registered cursors.
                        applier.resetCursors();

                        // New redo logs begin with identifiers one higher than last scanned.
                        mRedoWriter = new _RedoLog(config, replayLog, mTxnContexts[0]);

                        // TODO: If any exception is thrown before checkpoint is complete,
                        // delete the newly created redo log file.

                        if (doCheckpoint) {
                            // Do this early for checkpoint to store correct transaction id.
                            resetTransactionContexts(txnId);
                            txnId = -1;

                            checkpoint(true, 0, 0);

                            // Only cleanup after successful checkpoint.
                            for (File file : redoFiles) {
                                file.delete();
                            }
                        }

                        // Delete lingering fragmented values after undo logs have been
                        // processed, ensuring deletes were committed.
                        emptyAllFragmentedTrash(true);
                    }

                    recoveryComplete(recoveryStart);
                }
            }

            if (txnId >= 0) {
                resetTransactionContexts(txnId);
            }

            if (mBaseFile == null || openMode == OPEN_TEMP || debugListener != null) {
                mTempFileManager = null;
            } else {
                mTempFileManager = new TempFileManager(mBaseFile, config.mFileFactory);
            }
        } catch (Throwable e) {
            // Close, but don't double report the exception since construction never finished.
            closeQuietly(this);
            throw e;
        }
    }

    /**
     * Post construction, allow additional threads access to the database.
     */
    private void finishInit(DatabaseConfig config) throws IOException {
        if (mRedoWriter == null && mTempFileManager == null) {
            // Nothing is durable and nothing to ever clean up.
            return;
        }

        Checkpointer c = new Checkpointer(this, config);
        mCheckpointer = c;

        // Register objects to automatically shutdown.
        c.register(new RedoClose(this));
        c.register(mTempFileManager);

        if (mRedoWriter instanceof _ReplRedoWriter) {
            // Need to do this after mRedoWriter is assigned, ensuring that trees are opened as
            // _TxnTree instances.
            applyCachePrimer(config);
        }

        if (config.mCachePriming && mPageDb.isDurable() && !mReadOnly) {
            c.register(new ShutdownPrimer(this));
        }

        // Must tag the trashed trees before starting replication and recovery. Otherwise,
        // trees recently deleted might get double deleted.
        _Tree trashed = openNextTrashedTree(null);

        if (trashed != null) {
            Thread deletion = new Thread
                (new Deletion(trashed, true, mEventListener), "IndexDeletion");
            deletion.setDaemon(true);
            deletion.start();
        }

        if (mRecoveryHandler != null) {
            mRecoveryHandler.init(this);
        }

        boolean initialCheckpoint = false;

        if (mRedoWriter instanceof _ReplRedoController) {
            // Start replication and recovery.
            _ReplRedoController controller = (_ReplRedoController) mRedoWriter;

            try {
                controller.ready(config.mReplInitialTxnId, new ReplicationManager.Accessor() {
                    @Override
                    public void notify(EventType type, String message, Object... args) {
                        EventListener listener = mEventListener;
                        if (listener != null) {
                            listener.notify(type, message, args);
                        }
                    }

                    @Override
                    public Database database() {
                        return _LocalDatabase.this;
                    }

                    @Override
                    public long control(byte[] message) throws IOException {
                        return writeControlMessage(message);
                    }
                });
            } catch (Throwable e) {
                closeQuietly(this, e);
                throw e;
            }

            recoveryComplete(config.mReplRecoveryStartNanos);
            initialCheckpoint = true;
        }

        c.start(initialCheckpoint);

        LHashTable.Obj<_LocalTransaction> txns = mRecoveredTransactions;
        if (txns != null) {
            new Thread(() -> {
                invokeRecoveryHandler(txns, mRedoWriter);
            }).start();
            mRecoveredTransactions = null;
        }
    }

    private long writeControlMessage(byte[] message) throws IOException {
        // Commit lock must be held to prevent a checkpoint from starting. If the control
        // message fails to be applied, panic the database. If the database is kept open after
        // a failure and then a checkpoint completes, the control message would be dropped.
        // Normal transactional operations aren't so sensitive, because they have an undo log.
        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            _RedoWriter redo = txnRedoWriter();
            _TransactionContext context = anyTransactionContext();
            long commitPos = context.redoControl(redo, message);

            // Waiting for confirmation with the shared lock held isn't ideal, but control
            // messages aren't that frequent.
            redo.commitSync(context, commitPos);

            try {
                ((_ReplRedoController) mRedoWriter).mManager.control(commitPos, message);
            } catch (Throwable e) {
                // Panic.
                closeQuietly(this, e);
                throw e;
            }

            return commitPos;
        } finally {
            shared.release();
        }
    }

    private void applyCachePrimer(DatabaseConfig config) {
        if (mPageDb.isDurable()) {
            File primer = primerFile();
            try {
                if (config.mCachePriming && primer.exists()) {
                    if (mEventListener != null) {
                        mEventListener.notify(EventType.RECOVERY_CACHE_PRIMING,
                                              "Cache priming");
                    }
                    FileInputStream fin;
                    try {
                        fin = new FileInputStream(primer);
                        try (InputStream bin = new BufferedInputStream(fin)) {
                            applyCachePrimer(bin);
                        } catch (IOException e) {
                            fin.close();
                        }
                    } catch (IOException e) {
                    }
                }
            } finally {
                if (!mReadOnly) {
                    primer.delete();
                }
            }
        }
    }

    /**
     * @return true if a recovery handler exists and should be invoked
     */
    boolean shouldInvokeRecoveryHandler(LHashTable.Obj<_LocalTransaction> txns) {
        if (txns != null && txns.size() != 0) {
            if (mRecoveryHandler != null) {
                return true;
            }
            if (mEventListener != null) {
                mEventListener.notify
                    (EventType.RECOVERY_NO_HANDLER,
                     "No handler is installed for processing the remaining " +
                     "two-phase commit transactions: %1$d", txns.size());
            }
        }

        return false;
    }

    /**
     * To be called only when shouldInvokeRecoveryHandler returns true.
     *
     * @param redo non-null _RedoWriter assigned to each transaction
     */
    void invokeRecoveryHandler(LHashTable.Obj<_LocalTransaction> txns, _RedoWriter redo) {
        RecoveryHandler handler = mRecoveryHandler;

        txns.traverse(entry -> {
            _LocalTransaction txn = entry.value;
            txn.recoverPrepared
                (redo, mDurabilityMode, LockMode.UPGRADABLE_READ, mDefaultLockTimeoutNanos);

            try {
                handler.recover(txn);
            } catch (Throwable e) {
                if (!isClosed()) {
                    EventListener listener = mEventListener;
                    if (listener == null) {
                        uncaught(e);
                    } else {
                        listener.notify(EventType.RECOVERY_HANDLER_UNCAUGHT,
                                        "Uncaught exception from recovery handler: %1$s", e);
                    }
                }
            }

            return true;
        });
    }

    static class ShutdownPrimer extends ShutdownHook.Weak<_LocalDatabase> {
        ShutdownPrimer(_LocalDatabase db) {
            super(db);
        }

        @Override
        void doShutdown(_LocalDatabase db) {
            if (db.mReadOnly) {
                return;
            }

            File primer = db.primerFile();

            FileOutputStream fout;
            try {
                fout = new FileOutputStream(primer);
                try {
                    try (OutputStream bout = new BufferedOutputStream(fout)) {
                        db.createCachePrimer(bout);
                    }
                } catch (IOException e) {
                    fout.close();
                    primer.delete();
                }
            } catch (IOException e) {
            }
        }
    }

    File primerFile() {
        return new File(mBaseFile.getPath() + PRIMER_FILE_SUFFIX);
    }

    private void recoveryComplete(long recoveryStart) {
        if (mEventListener != null) {
            double duration = (System.nanoTime() - recoveryStart) / 1_000_000_000.0;
            mEventListener.notify(EventType.RECOVERY_COMPLETE,
                                  "Recovery completed in %1$1.3f seconds",
                                  duration, TimeUnit.SECONDS);
        }
    }

    private void deleteRedoLogFiles() throws IOException {
        if (mBaseFile != null && !mReadOnly) {
            deleteNumberedFiles(mBaseFile, REDO_FILE_SUFFIX);
        }
    }

    @Override
    public Index findIndex(byte[] name) throws IOException {
        return openTree(name.clone(), false);
    }

    @Override
    public Index openIndex(byte[] name) throws IOException {
        return openTree(name.clone(), true);
    }

    @Override
    public Index indexById(long id) throws IOException {
        return indexById(null, id);
    }

    Index indexById(Transaction txn, long id) throws IOException {
        if (_Tree.isInternal(id)) {
            throw new IllegalArgumentException("Invalid id: " + id);
        }

        Index index;

        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            if ((index = lookupIndexById(id)) != null) {
                return index;
            }

            byte[] idKey = new byte[9];
            idKey[0] = KEY_TYPE_INDEX_ID;
            encodeLongBE(idKey, 1, id);

            byte[] name;

            if (txn != null) {
                name = mRegistryKeyMap.load(txn, idKey);
            } else {
                // Lookup name with exclusive lock, to prevent races with concurrent index
                // creation. If a replicated operation which requires the newly created index
                // merely acquired a shared lock, then it might not find the index at all.
                _Locker locker = mRegistryKeyMap.lockExclusiveLocal
                    (idKey, _LockManager.hash(mRegistryKeyMap.getId(), idKey));
                try {
                    name = mRegistryKeyMap.load(Transaction.BOGUS, idKey);
                } finally {
                    locker.unlock();
                }
            }

            if (name == null) {
                checkClosed();
                return null;
            }

            byte[] treeIdBytes = new byte[8];
            encodeLongBE(treeIdBytes, 0, id);            

            index = openTree(txn, treeIdBytes, name, false);
        } catch (Throwable e) {
            DatabaseException.rethrowIfRecoverable(e);
            throw closeOnFailure(this, e);
        } finally {
            shared.release();
        }

        if (index == null) {
            // Registry needs to be repaired to fix this.
            throw new DatabaseException("Unable to find index in registry");
        }

        return index;
    }

    /**
     * @return null if index is not open
     */
    private _Tree lookupIndexById(long id) {
        mOpenTreesLatch.acquireShared();
        try {
            LHashTable.ObjEntry<_TreeRef> entry = mOpenTreesById.get(id);
            return entry == null ? null : entry.value.get();
        } finally {
            mOpenTreesLatch.releaseShared();
        }
    }

    /**
     * Allows access to internal indexes which can use the redo log.
     */
    Index anyIndexById(long id) throws IOException {
        return anyIndexById(null, id);
    }

    /**
     * Allows access to internal indexes which can use the redo log.
     */
    Index anyIndexById(Transaction txn, long id) throws IOException {
        if (id == _Tree.REGISTRY_KEY_MAP_ID) {
            return mRegistryKeyMap;
        } else if (id == _Tree.FRAGMENTED_TRASH_ID) {
            return fragmentedTrash().mTrash;
        }
        return indexById(txn, id);
    }

    @Override
    public void renameIndex(Index index, byte[] newName) throws IOException {
        renameIndex(index, newName.clone(), 0);
    }

    /**
     * @param newName not cloned
     * @param redoTxnId non-zero if rename is performed by recovery
     */
    void renameIndex(final Index index, final byte[] newName, final long redoTxnId)
        throws IOException
    {
        // Design note: Rename is a Database method instead of an Index method because it
        // offers an extra degree of safety. It's too easy to call rename and pass a byte[] by
        // an accident when something like remove was desired instead. Requiring access to the
        // Database instance makes this operation a bit more of a hassle to use, which is
        // desirable. Rename is not expected to be a common operation.

        final _Tree tree = accessTree(index);

        final byte[] idKey, trashIdKey;
        final byte[] oldName, oldNameKey;
        final byte[] newNameKey;

        final _LocalTransaction txn;

        final _Node root = tree.mRoot;
        root.acquireExclusive();
        try {
            if (root.mPage == p_closedTreePage()) {
                throw new ClosedIndexException();
            }

            if (_Tree.isInternal(tree.mId)) {
                throw new IllegalStateException("Cannot rename an internal index");
            }

            oldName = tree.mName;

            if (oldName == null) {
                throw new IllegalStateException("Cannot rename a temporary index");
            }

            if (Arrays.equals(oldName, newName)) {
                return;
            }

            idKey = newKey(KEY_TYPE_INDEX_ID, tree.mIdBytes);
            trashIdKey = newKey(KEY_TYPE_TRASH_ID, tree.mIdBytes);
            oldNameKey = newKey(KEY_TYPE_INDEX_NAME, oldName);
            newNameKey = newKey(KEY_TYPE_INDEX_NAME, newName);

            txn = newNoRedoTransaction(redoTxnId);
            try {
                txn.lockTimeout(-1, null);
                txn.lockExclusive(mRegistryKeyMap.mId, idKey);
                txn.lockExclusive(mRegistryKeyMap.mId, trashIdKey);
                // _Lock in a consistent order, avoiding deadlocks.
                if (compareUnsigned(oldNameKey, newNameKey) <= 0) {
                    txn.lockExclusive(mRegistryKeyMap.mId, oldNameKey);
                    txn.lockExclusive(mRegistryKeyMap.mId, newNameKey);
                } else {
                    txn.lockExclusive(mRegistryKeyMap.mId, newNameKey);
                    txn.lockExclusive(mRegistryKeyMap.mId, oldNameKey);
                }
            } catch (Throwable e) {
                txn.reset();
                throw e;
            }
        } finally {
            // Can release now that registry entries are locked. Those locks will prevent
            // concurrent renames of the same index.
            root.releaseExclusive();
        }

        try {
            Cursor c = mRegistryKeyMap.newCursor(txn);
            try {
                c.autoload(false);

                c.find(trashIdKey);
                if (c.value() != null) {
                    throw new IllegalStateException("Index is deleted");
                }

                c.find(newNameKey);
                if (c.value() != null) {
                    throw new IllegalStateException("New name is used by another index");
                }

                c.store(tree.mIdBytes);
            } finally {
                c.reset();
            }

            if (redoTxnId == 0 && txn.mRedo != null) {
                txn.durabilityMode(mDurabilityMode.alwaysRedo());

                long commitPos;
                CommitLock.Shared shared = mCommitLock.acquireShared();
                try {
                    txn.check();
                    commitPos = txn.mContext.redoRenameIndexCommitFinal
                        (txn.mRedo, txn.txnId(), tree.mId, newName, txn.durabilityMode());
                } finally {
                    shared.release();
                }

                if (commitPos != 0) {
                    // Must wait for durability confirmation before performing actions below
                    // which cannot be easily rolled back. No global latches or locks are held
                    // while waiting.
                    txn.mRedo.txnCommitSync(txn, commitPos);
                }
            }

            mRegistryKeyMap.delete(txn, oldNameKey);
            mRegistryKeyMap.store(txn, idKey, newName);

            mOpenTreesLatch.acquireExclusive();
            try {
                txn.commit();

                tree.mName = newName;
                mOpenTrees.put(newName, mOpenTrees.remove(oldName));
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }
        } catch (IllegalStateException e) {
            throw e;
        } catch (Throwable e) {
            DatabaseException.rethrowIfRecoverable(e);
            throw closeOnFailure(this, e);
        } finally {
            txn.reset();
        }
    }

    private _Tree accessTree(Index index) {
        try {
            _Tree tree;
            if ((tree = ((_Tree) index)).mDatabase == this) {
                return tree;
            }
        } catch (ClassCastException e) {
            // Cast and catch an exception instead of calling instanceof to cause a
            // NullPointerException to be thrown if index is null.
        }
        throw new IllegalArgumentException("Index belongs to a different database");
    }

    @Override
    public Runnable deleteIndex(Index index) throws IOException {
        // Design note: This is a Database method instead of an Index method because it offers
        // an extra degree of safety. See notes in renameIndex.
        return accessTree(index).drop(false);
    }

    /**
     * Returns a deletion task for a tree which just moved to the trash.
     */
    Runnable replicaDeleteTree(long treeId) throws IOException {
        byte[] treeIdBytes = new byte[8];
        encodeLongBE(treeIdBytes, 0, treeId);

        _Tree trashed = openTrashedTree(treeIdBytes, false);

        return new Deletion(trashed, false, null);
    }

    /**
     * Called by _Tree.drop with root node latch held exclusively.
     *
     * @param shared commit lock held shared; always released by this method
     */
    Runnable deleteTree(_Tree tree, CommitLock.Shared shared) throws IOException {
        try {
            if (!(tree instanceof _TempTree) && !moveToTrash(tree.mId, tree.mIdBytes)) {
                // Handle concurrent delete attempt.
                throw new ClosedIndexException();
            }
        } finally {
            // Always release before calling close, which might require an exclusive lock.
            shared.release();
        }

        _Node root = tree.close(true, true);
        if (root == null) {
            // Handle concurrent close attempt.
            throw new ClosedIndexException();
        }

        _Tree trashed = newTreeInstance(tree.mId, tree.mIdBytes, tree.mName, root);

        return new Deletion(trashed, false, null);
    }

    /**
     * Quickly delete an empty temporary tree, which has no active threads and cursors.
     */
    void quickDeleteTemporaryTree(_Tree tree) throws IOException {
        mOpenTreesLatch.acquireExclusive();
        try {
            _TreeRef ref = mOpenTreesById.removeValue(tree.mId);
            if (ref == null || ref.get() != tree) {
                // _Tree is likely being closed by a concurrent database close.
                return;
            }
            ref.clear();
        } finally {
            mOpenTreesLatch.releaseExclusive();
        }

        _Node root = tree.mRoot;

        prepare: {
            CommitLock.Shared shared = mCommitLock.acquireShared();
            try {
                root.acquireExclusive();
                if (!root.hasKeys()) {
                    prepareToDelete(root);
                    root.releaseExclusive();
                    break prepare;
                }
                root.releaseExclusive();
            } finally {
                shared.release();
            }

            // _Tree isn't truly empty -- it might be composed of many empty leaf nodes.
            tree.deleteAll();
        }

        removeFromTrash(tree, root);
    }

    /**
     * @param lastIdBytes null to start with first
     * @return null if none available
     */
    private _Tree openNextTrashedTree(byte[] lastIdBytes) throws IOException {
        return openTrashedTree(lastIdBytes, true);
    }

    /**
     * @param idBytes null to start with first
     * @param next true to find tree with next higher id
     * @return null if not found
     */
    private _Tree openTrashedTree(byte[] idBytes, boolean next) throws IOException {
        View view = mRegistryKeyMap.viewPrefix(new byte[] {KEY_TYPE_TRASH_ID}, 1);

        if (idBytes == null) {
            // Tag all the entries that should be deleted automatically. Entries created later
            // will have a different prefix, and so they'll be ignored.
            Cursor c = view.newCursor(Transaction.BOGUS);
            try {
                for (c.first(); c.key() != null; c.next()) {
                    byte[] name = c.value();
                    if (name.length != 0) {
                        name[0] |= 0x80;
                        c.store(name);
                    }
                }
            } finally {
                c.reset();
            }
        }

        byte[] treeIdBytes, name, rootIdBytes;

        Cursor c = view.newCursor(Transaction.BOGUS);
        try {
            if (idBytes == null) {
                c.first();
            } else if (next) {
                c.findGt(idBytes);
            } else {
                c.find(idBytes);
            }

            while (true) {
                treeIdBytes = c.key();

                if (treeIdBytes == null) {
                    return null;
                }

                rootIdBytes = mRegistry.load(Transaction.BOGUS, treeIdBytes);

                if (rootIdBytes == null) {
                    // Clear out bogus entry in the trash.
                    c.store(null);
                } else {
                    name = c.value();
                    if (name[0] < 0) {
                        // Found a tagged entry.
                        break;
                    }
                }

                if (next) {
                    c.next();
                } else {
                    return null;
                }
            }
        } finally {
            c.reset();
        }

        long rootId = rootIdBytes.length == 0 ? 0 : decodeLongLE(rootIdBytes, 0);

        if ((name[0] & 0x7f) == 0) {
            name = null;
        } else {
            // Trim off the tag byte.
            byte[] actual = new byte[name.length - 1];
            System.arraycopy(name, 1, actual, 0, actual.length);
            name = actual;
        }

        long treeId = decodeLongBE(treeIdBytes, 0);

        return newTreeInstance(treeId, treeIdBytes, name, loadTreeRoot(treeId, rootId));
    }

    private class Deletion implements Runnable {
        private _Tree mTrashed;
        private final boolean mResumed;
        private final EventListener mListener;

        Deletion(_Tree trashed, boolean resumed, EventListener listener) {
            mTrashed = trashed;
            mResumed = resumed;
            mListener = listener;
        }

        @Override
        public synchronized void run() {
            while (mTrashed != null) {
                delete();
            }
        }

        private void delete() {
            if (mListener != null) {
                mListener.notify(EventType.DELETION_BEGIN,
                                 "Index deletion " + (mResumed ? "resumed" : "begin") +
                                 ": %1$d, name: %2$s",
                                 mTrashed.getId(), mTrashed.getNameString());
            }

            final byte[] idBytes = mTrashed.mIdBytes;

            try {
                long start = System.nanoTime();

                if (mTrashed.deleteAll()) {
                    _Node root = mTrashed.close(true, false);
                    removeFromTrash(mTrashed, root);
                } else {
                    // Database is closed.
                    return;
                }

                if (mListener != null) {
                    double duration = (System.nanoTime() - start) / 1_000_000_000.0;
                    mListener.notify(EventType.DELETION_COMPLETE,
                                     "Index deletion complete: %1$d, name: %2$s, " +
                                     "duration: %3$1.3f seconds",
                                     mTrashed.getId(), mTrashed.getNameString(), duration);
                }
            } catch (IOException e) {
                if (!isClosed() && mListener != null) {
                    mListener.notify
                        (EventType.DELETION_FAILED,
                         "Index deletion failed: %1$d, name: %2$s, exception: %3$s",
                         mTrashed.getId(), mTrashed.getNameString(), rootCause(e));
                }
                closeQuietly(mTrashed);
                return;
            } finally {
                mTrashed = null;
            }

            if (mResumed) {
                try {
                    mTrashed = openNextTrashedTree(idBytes);
                } catch (IOException e) {
                    if (!isClosed() && mListener != null) {
                        mListener.notify
                            (EventType.DELETION_FAILED,
                             "Unable to resume deletion: %1$s", rootCause(e));
                    }
                    return;
                }
            }
        }
    }

    @Override
    public _Tree newTemporaryIndex() throws IOException {
        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            return newTemporaryTree(false);
        } finally {
            shared.release();
        }
    }

    /**
     * Caller must hold commit lock. Pass true to preallocate a dirty root node for the tree,
     * which will be held exclusive. Caller is then responsible for initializing it
     */
    _Tree newTemporaryTree(boolean preallocate) throws IOException {
        checkClosed();

        // Cleanup before opening more trees.
        cleanupUnreferencedTrees();

        long treeId;
        byte[] treeIdBytes = new byte[8];

        long rootId;
        byte[] rootIdBytes;

        if (preallocate) {
            rootId = mPageDb.allocPage();
            rootIdBytes = new byte[8];
        } else {
            rootId = 0;
            rootIdBytes = EMPTY_BYTES;
        }

        try {
            do {
                treeId = nextTreeId(true);
                encodeLongBE(treeIdBytes, 0, treeId);
            } while (!mRegistry.insert(Transaction.BOGUS, treeIdBytes, rootIdBytes));

            // Register temporary index as trash, unreplicated.
            Transaction createTxn = newNoRedoTransaction();
            try {
                createTxn.lockTimeout(-1, null);
                byte[] trashIdKey = newKey(KEY_TYPE_TRASH_ID, treeIdBytes);
                if (!mRegistryKeyMap.insert(createTxn, trashIdKey, new byte[1])) {
                    throw new DatabaseException("Unable to register temporary index");
                }
                createTxn.commit();
            } finally {
                createTxn.reset();
            }

            _Node root;
            if (rootId != 0) {
                root = allocLatchedNode(rootId, _NodeContext.MODE_UNEVICTABLE);
                root.mId = rootId;
                try {
                    // Note: Same as redirty method, except mPage is assigned when fully mapped.
                    // The redirty method assumes that the page doesn't change.
                    /*P*/ // [|
                    if (mFullyMapped) {
                        root.mPage = mPageDb.dirtyPage(rootId);
                    }
                    /*P*/ // ]
                    root.mContext.addDirty(root, mCommitState);
                } catch (Throwable e) {
                    root.releaseExclusive();
                    throw e;
                }
            } else {
                root = loadTreeRoot(treeId, 0);
            }

            try {
                _Tree tree = new _TempTree(this, treeId, treeIdBytes, root);
                _TreeRef treeRef = new _TreeRef(tree, mOpenTreesRefQueue);

                mOpenTreesLatch.acquireExclusive();
                try {
                    mOpenTreesById.insert(treeId).value = treeRef;
                } finally {
                    mOpenTreesLatch.releaseExclusive();
                }

                return tree;
            } catch (Throwable e) {
                if (rootId != 0) {
                    root.releaseExclusive();
                }
                throw e;
            }
        } catch (Throwable e) {
            try {
                mRegistry.delete(Transaction.BOGUS, treeIdBytes);
            } catch (Throwable e2) {
                // Panic.
                throw closeOnFailure(this, e);
            }
            if (rootId != 0) {
                try {
                    mPageDb.recyclePage(rootId);
                } catch (Throwable e2) {
                    Utils.suppress(e, e2);
                }
            }
            throw e;
        }
    }

    @Override
    public View indexRegistryByName() throws IOException {
        return mRegistryKeyMap.viewPrefix(new byte[] {KEY_TYPE_INDEX_NAME}, 1).viewUnmodifiable();
    }

    @Override
    public View indexRegistryById() throws IOException {
        return mRegistryKeyMap.viewPrefix(new byte[] {KEY_TYPE_INDEX_ID}, 1).viewUnmodifiable();
    }

    @Override
    public Transaction newTransaction() {
        return doNewTransaction(mDurabilityMode);
    }

    @Override
    public Transaction newTransaction(DurabilityMode durabilityMode) {
        return doNewTransaction(durabilityMode == null ? mDurabilityMode : durabilityMode);
    }

    private _LocalTransaction doNewTransaction(DurabilityMode durabilityMode) {
        _RedoWriter redo = txnRedoWriter();
        return new _LocalTransaction
            (this, redo, durabilityMode, LockMode.UPGRADABLE_READ, mDefaultLockTimeoutNanos);
    }

    private _LocalTransaction newAlwaysRedoTransaction() {
        return doNewTransaction(mDurabilityMode.alwaysRedo());
    }

    /**
     * Convenience method which returns a transaction intended for locking and undo. Caller can
     * make modifications, but they won't go to the redo log.
     */
    private _LocalTransaction newNoRedoTransaction() {
        return doNewTransaction(DurabilityMode.NO_REDO);
    }

    /**
     * Convenience method which returns a transaction intended for locking and undo. Caller can
     * make modifications, but they won't go to the redo log.
     *
     * @param redoTxnId non-zero if operation is performed by recovery
     */
    private _LocalTransaction newNoRedoTransaction(long redoTxnId) {
        return redoTxnId == 0 ? newNoRedoTransaction() :
            new _LocalTransaction(this, redoTxnId, LockMode.UPGRADABLE_READ,
                                 mDefaultLockTimeoutNanos);
    }

    /**
     * Returns a transaction which should be briefly used and reset.
     */
    _LocalTransaction threadLocalTransaction(DurabilityMode durabilityMode) {
        SoftReference<_LocalTransaction> txnRef = mLocalTransaction.get();
        _LocalTransaction txn;
        if (txnRef == null || (txn = txnRef.get()) == null) {
            txn = doNewTransaction(durabilityMode);
            mLocalTransaction.set(new SoftReference<>(txn));
        } else {
            txn.mRedo = txnRedoWriter();
            txn.mDurabilityMode = durabilityMode;
            txn.mLockMode = LockMode.UPGRADABLE_READ;
            txn.mLockTimeoutNanos = mDefaultLockTimeoutNanos;
        }
        return txn;
    }

    void removeThreadLocalTransaction() {
        mLocalTransaction.remove();
    }

    /**
     * Returns a _RedoWriter suitable for transactions to write into.
     */
    _RedoWriter txnRedoWriter() {
        _RedoWriter redo = mRedoWriter;
        if (redo != null) {
            redo = redo.txnRedoWriter();
        }
        return redo;
    }

    private void resetTransactionContexts(long txnId) {
        for (_TransactionContext txnContext : mTxnContexts) {
            txnContext.resetTransactionId(txnId++);
        }
    }

    /**
     * Used by auto-commit operations that don't have an explicit transaction.
     */
    _TransactionContext anyTransactionContext() {
        return selectTransactionContext(ThreadLocalRandom.current().nextInt());
    }

    /**
     * Called by transaction constructor after hash code has been assigned.
     */
    _TransactionContext selectTransactionContext(_LocalTransaction txn) {
        return selectTransactionContext(txn.hashCode());
    }

    private _TransactionContext selectTransactionContext(int num) {
        return mTxnContexts[(num & 0x7fffffff) % mTxnContexts.length];
    }

    @Override
    public long preallocate(long bytes) throws IOException {
        if (!isClosed() && mPageDb.isDurable()) {
            int pageSize = mPageSize;
            long pageCount = (bytes + pageSize - 1) / pageSize;
            if (pageCount > 0) {
                pageCount = mPageDb.allocatePages(pageCount);
                if (pageCount > 0) {
                    try {
                        checkpoint(true, 0, 0);
                    } catch (Throwable e) {
                        DatabaseException.rethrowIfRecoverable(e);
                        closeQuietly(this, e);
                        throw e;
                    }
                }
                return pageCount * pageSize;
            }
        }
        return 0;
    }

    @Override
    public Sorter newSorter(Executor executor) throws IOException {
        if (executor == null && (executor = mSorterExecutor) == null) {
            mOpenTreesLatch.acquireExclusive();
            try {
                checkClosed();
                executor = mSorterExecutor;
                if (executor == null) {
                    ExecutorService es = Executors.newCachedThreadPool(r -> {
                        Thread t = new Thread(r);
                        t.setDaemon(true);
                        t.setName("Sorter-" + Long.toUnsignedString(t.getId()));
                        return t;
                    });
                    mSorterExecutor = es;
                    executor = es;
                }
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }
        }

        return new _ParallelSorter(this, executor);
    }

    @Override
    public void capacityLimit(long bytes) {
        mPageDb.pageLimit(bytes < 0 ? -1 : (bytes / mPageSize));
    }

    @Override
    public long capacityLimit() {
        long pageLimit = mPageDb.pageLimit();
        return pageLimit < 0 ? -1 : (pageLimit * mPageSize);
    }

    @Override
    public void capacityLimitOverride(long bytes) {
        mPageDb.pageLimitOverride(bytes < 0 ? -1 : (bytes / mPageSize));
    }

    @Override
    public Snapshot beginSnapshot() throws IOException {
        if (!(mPageDb.isDurable())) {
            throw new UnsupportedOperationException("Snapshot only allowed for durable databases");
        }
        checkClosed();
        _DurablePageDb pageDb = (_DurablePageDb) mPageDb;
        return pageDb.beginSnapshot(this);
    }

    /**
     * Restore from a {@link #beginSnapshot snapshot}, into the data files defined by the given
     * configuration. All existing data and redo log files at the snapshot destination are
     * deleted before the restore begins.
     *
     * @param in snapshot source; does not require extra buffering; auto-closed
     */
    static Database restoreFromSnapshot(DatabaseConfig config, InputStream in) throws IOException {
        if (config.mReadOnly) {
            throw new IllegalArgumentException("Cannot restore into a read-only database");
        }

        config = config.clone();
        _PageDb restored;

        File[] dataFiles = config.dataFiles();
        if (dataFiles == null) {
            PageArray dataPageArray = config.mDataPageArray;

            if (dataPageArray == null) {
                throw new UnsupportedOperationException
                    ("Restore only allowed for durable databases");
            }

            dataPageArray = dataPageArray.open();
            dataPageArray.setPageCount(0);

            // Delete old redo log files.
            deleteNumberedFiles(config.mBaseFile, REDO_FILE_SUFFIX);

            restored = _DurablePageDb.restoreFromSnapshot(dataPageArray, null, config.mCrypto, in);
        } else {
            for (File f : dataFiles) {
                // Delete old data file.
                f.delete();
                if (config.mMkdirs) {
                    f.getParentFile().mkdirs();
                }
            }

            FileFactory factory = config.mFileFactory;
            EnumSet<OpenOption> options = config.createOpenOptions();

            // Delete old redo log files.
            deleteNumberedFiles(config.mBaseFile, REDO_FILE_SUFFIX);

            int pageSize = config.mPageSize;
            if (pageSize <= 0) {
                pageSize = DEFAULT_PAGE_SIZE;
            }

            restored = _DurablePageDb.restoreFromSnapshot
                (pageSize, dataFiles, factory, options, null, config.mCrypto, in);
        }

        try {
            restored.close();
        } finally {
            restored.delete();
        }

        return Database.open(config);
    }

    @Override
    public void createCachePrimer(OutputStream out) throws IOException {
        if (!(mPageDb.isDurable())) {
            throw new UnsupportedOperationException
                ("Cache priming only allowed for durable databases");
        }

        out = ((_DurablePageDb) mPageDb).encrypt(out);

        DataOutputStream dout = new DataOutputStream(out);

        dout.writeLong(PRIMER_MAGIC_NUMBER);

        for (_TreeRef treeRef : mOpenTrees.values()) {
            _Tree tree = treeRef.get();
            if (tree != null && !_Tree.isInternal(tree.mId)) {
                // Encode name instead of identifier, to support priming set portability
                // between databases. The identifiers won't match, but the names might.
                byte[] name = tree.mName;
                dout.writeInt(name.length);
                dout.write(name);
                tree.writeCachePrimer(dout);
            }
        }

        // Terminator.
        dout.writeInt(-1);
    }

    @Override
    public void applyCachePrimer(InputStream in) throws IOException {
        if (!(mPageDb.isDurable())) {
            throw new UnsupportedOperationException
                ("Cache priming only allowed for durable databases");
        }

        in = ((_DurablePageDb) mPageDb).decrypt(in);

        DataInput din;
        if (in instanceof DataInput) {
            din = (DataInput) in;
        } else {
            din = new DataInputStream(in);
        }

        long magic = din.readLong();
        if (magic != PRIMER_MAGIC_NUMBER) {
            throw new DatabaseException("Wrong cache primer magic number: " + magic);
        }

        while (true) {
            int len = din.readInt();
            if (len < 0) {
                break;
            }
            byte[] name = new byte[len];
            din.readFully(name);
            _Tree tree = openTree(name, false);
            if (tree != null) {
                tree.applyCachePrimer(din);
            } else {
                _Tree.skipCachePrimer(din);
            }
        }
    }

    @Override
    public Stats stats() {
        Stats stats = new Stats();

        stats.pageSize = mPageSize;

        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            long cursorCount = 0;
            int openTreesCount = 0;

            for (_TreeRef treeRef : mOpenTrees.values()) {
                _Tree tree = treeRef.get();
                if (tree != null) {
                    openTreesCount++;
                    cursorCount += tree.mRoot.countCursors();
                }
            }

            cursorCount += mRegistry.mRoot.countCursors();
            cursorCount += mRegistryKeyMap.mRoot.countCursors();

            _FragmentedTrash trash = mFragmentedTrash;
            if (trash != null) {
                cursorCount += trash.mTrash.mRoot.countCursors();
            }

            _Tree cursorRegistry = mCursorRegistry;
            if (cursorRegistry != null) {
                // Count the cursors which are actively registering cursors. Sounds confusing.
                cursorCount += cursorRegistry.mRoot.countCursors();
            }

            stats.openIndexes = openTreesCount;
            stats.cursorCount = cursorCount;

            _PageDb.Stats pstats = mPageDb.stats();
            stats.freePages = pstats.freePages;
            stats.totalPages = pstats.totalPages;

            stats.lockCount = mLockManager.numLocksHeld();

            for (_TransactionContext txnContext : mTxnContexts) {
                txnContext.addStats(stats);
            }
        } finally {
            shared.release();
        }

        for (_NodeContext context : mNodeContexts) {
            stats.cachedPages += context.nodeCount();
            stats.dirtyPages += context.dirtyCount();
        }

        if (stats.dirtyPages > stats.totalPages) {
            stats.dirtyPages = stats.totalPages;
        }

        return stats;
    }

    static class RedoClose extends ShutdownHook.Weak<_LocalDatabase> {
        RedoClose(_LocalDatabase db) {
            super(db);
        }

        @Override
        void doShutdown(_LocalDatabase db) {
            db.redoClose(RedoOps.OP_SHUTDOWN, null);
        }
    }

    /**
     * @param op OP_CLOSE or OP_SHUTDOWN
     */
    private void redoClose(byte op, Throwable cause) {
        _RedoWriter redo = mRedoWriter;
        if (redo == null) {
            return;
        }

        redo.closeCause(cause);
        redo = redo.txnRedoWriter();
        redo.closeCause(cause);

        try {
            // NO_FLUSH now behaves like NO_SYNC.
            redo.alwaysFlush(true);
        } catch (IOException e) {
            // Ignore.
        }

        try {
            _TransactionContext context = anyTransactionContext();
            context.redoTimestamp(redo, op);
            context.flush();

            redo.force(true);
        } catch (IOException e) {
            // Ignore.
        }

        // When shutdown hook is invoked, don't close the redo writer. It may interfere with
        // other shutdown hooks, causing unexpected exceptions to be thrown during the whole
        // shutdown sequence.

        if (op == RedoOps.OP_CLOSE) {
            Utils.closeQuietly(redo);
        }
    }

    @Override
    public void flush() throws IOException {
        flush(0); // flush only
    }

    @Override
    public void sync() throws IOException {
        flush(1); // flush and sync
    }

    /**
     * @param level 0: flush only, 1: flush and sync, 2: flush and sync metadata
     */
    private void flush(int level) throws IOException {
        if (!isClosed() && mRedoWriter != null) {
            mRedoWriter.flush();
            if (level > 0) {
                mRedoWriter.force(level > 1);
            }
        }
    }

    @Override
    public void checkpoint() throws IOException {
        while (!isClosed() && mPageDb.isDurable()) {
            try {
                checkpoint(false, 0, 0);
                return;
            } catch (UnmodifiableReplicaException e) {
                // Retry.
                Thread.yield();
            } catch (Throwable e) {
                DatabaseException.rethrowIfRecoverable(e);
                closeQuietly(this, e);
                throw e;
            }
        }
    }

    @Override
    public void suspendCheckpoints() {
        Checkpointer c = mCheckpointer;
        if (c != null) {
            c.suspend();
        }
    }

    @Override
    public void resumeCheckpoints() {
        Checkpointer c = mCheckpointer;
        if (c != null) {
            c.resume();
        }
    }

    @Override
    public boolean compactFile(CompactionObserver observer, double target) throws IOException {
        if (target < 0 || target > 1) {
            throw new IllegalArgumentException("Illegal compaction target: " + target);
        }

        if (target == 0) {
            // No compaction to do at all, but not aborted.
            return true;
        }

        long targetPageCount;
        mCheckpointLock.lock();
        try {
            _PageDb.Stats stats = mPageDb.stats();
            long usedPages = stats.totalPages - stats.freePages;
            targetPageCount = Math.max(usedPages, (long) (usedPages / target));

            // Determine the maximum amount of space required to store the reserve list nodes
            // and ensure the target includes them.
            long reserve;
            {
                // Total pages freed.
                long freed = stats.totalPages - targetPageCount;

                // Scale by the maximum size for encoding page identifiers, assuming no savings
                // from delta encoding.
                freed *= calcUnsignedVarLongLength(stats.totalPages << 1);

                // Divide by the node size, excluding the header (see _PageQueue).
                reserve = freed / (mPageSize - (8 + 8));

                // A minimum is required because the regular and free lists need to allocate
                // one extra node at checkpoint. Up to three checkpoints may be issued, so pad
                // by 2 * 3 = 6.
                reserve += 6;
            }

            targetPageCount += reserve;

            if (targetPageCount >= stats.totalPages && targetPageCount >= mPageDb.pageCount()) {
                return true;
            }

            if (!mPageDb.compactionStart(targetPageCount)) {
                return false;
            }
        } finally {
            mCheckpointLock.unlock();
        }

        boolean completed = mPageDb.compactionScanFreeList();

        if (completed) {
            // Issue a checkpoint to ensure all dirty nodes are flushed out. This ensures that
            // nodes can be moved out of the compaction zone by simply marking them dirty. If
            // already dirty, they'll not be in the compaction zone unless compaction aborted.
            checkpoint();

            if (observer == null) {
                observer = new CompactionObserver();
            }

            final long highestNodeId = targetPageCount - 1;
            final CompactionObserver fobserver = observer;

            completed = scanAllIndexes(tree -> {
                return tree.compactTree(tree.observableView(), highestNodeId, fobserver);
            });

            checkpoint(true, 0, 0);

            if (completed && mPageDb.compactionScanFreeList()) {
                if (!mPageDb.compactionVerify() && mPageDb.compactionScanFreeList()) {
                    checkpoint(true, 0, 0);
                }
            }
        }

        mCheckpointLock.lock();
        try {
            completed &= mPageDb.compactionEnd();

            // Reclaim reserved pages, but only after a checkpoint has been performed.
            checkpoint(true, 0, 0);
            mPageDb.compactionReclaim();
            // Checkpoint again in order for reclaimed pages to be immediately available.
            checkpoint(true, 0, 0);

            if (completed) {
                // And now, attempt to actually shrink the file.
                return mPageDb.truncatePages();
            }
        } finally {
            mCheckpointLock.unlock();
        }

        return false;
    }

    @Override
    public boolean verify(VerificationObserver observer) throws IOException {
        // TODO: Verify free lists.
        if (false) {
            mPageDb.scanFreeList(id -> System.out.println(id));
        }

        if (observer == null) {
            observer = new VerificationObserver();
        }

        final boolean[] passedRef = {true};
        final VerificationObserver fobserver = observer;

        scanAllIndexes(tree -> {
            Index view = tree.observableView();
            fobserver.failed = false;
            boolean keepGoing = tree.verifyTree(view, fobserver);
            passedRef[0] &= !fobserver.failed;
            if (keepGoing) {
                keepGoing = fobserver.indexComplete(view, !fobserver.failed, null);
            }
            return keepGoing;
        });

        return passedRef[0];
    }

    @FunctionalInterface
    static interface ScanVisitor {
        /**
         * @return false if should stop
         */
        boolean apply(_Tree tree) throws IOException;
    }

    /**
     * @return false if stopped
     */
    private boolean scanAllIndexes(ScanVisitor visitor) throws IOException {
        if (!visitor.apply(mRegistry)) {
            return false;
        }
        if (!visitor.apply(mRegistryKeyMap)) {
            return false;
        }

        _FragmentedTrash trash = mFragmentedTrash;
        if (trash != null) {
            if (!visitor.apply(trash.mTrash)) {
                return false;
            }
        }

        _Tree cursorRegistry = mCursorRegistry;
        if (cursorRegistry != null) {
            if (!visitor.apply(cursorRegistry)) {
                return false;
            }
        }

        Cursor all = indexRegistryByName().newCursor(null);
        try {
            for (all.first(); all.key() != null; all.next()) {
                long id = decodeLongBE(all.value(), 0);

                Index index = indexById(id);
                if (index instanceof _Tree && !visitor.apply((_Tree) index)) {
                    return false;
                }
            }
        } finally {
            all.reset();
        }

        return true;
    }

    @Override
    public void close(Throwable cause) throws IOException {
        close(cause, false);
    }

    @Override
    public void shutdown() throws IOException {
        close(null, mPageDb.isDurable());
    }

    private void close(Throwable cause, boolean shutdown) throws IOException {
        if (!cClosedUpdater.compareAndSet(this, 0, 1)) {
            return;
        }

        if (cause != null) {
            mClosedCause = cause;
            Throwable rootCause = rootCause(cause);
            if (mEventListener == null) {
                uncaught(rootCause);
            } else {
                mEventListener.notify(EventType.PANIC_UNHANDLED_EXCEPTION,
                                      "Closing database due to unhandled exception: %1$s",
                                      rootCause);
            }
        }

        Thread ct = null;
        boolean lockedCheckpointer = false;

        try {
            Checkpointer c = mCheckpointer;

            if (shutdown) {
                mCheckpointLock.lock();
                lockedCheckpointer = true;
                checkpoint(true, 0, 0);
                if (c != null) {
                    ct = c.close(cause);
                }
            } else {
                if (c != null) {
                    ct = c.close(cause);
                }

                // Wait for any in-progress checkpoint to complete.

                if (mCheckpointLock.tryLock()) {
                    lockedCheckpointer = true;
                } else if (cause == null && !(mRedoWriter instanceof _ReplRedoController)) {
                    // Only attempt lock if not panicked and not replicated. If panicked, other
                    // locks might be held and so acquiring checkpoint lock might deadlock.
                    // Replicated databases might stall indefinitely when checkpointing.
                    // Checkpointer should eventually exit after other resources are closed.
                    mCheckpointLock.lock();
                    lockedCheckpointer = true;
                }
            }
        } finally {
            if (ct != null) {
                ct.interrupt();
            }

            if (lockedCheckpointer) {
                mCheckpointLock.unlock();

                if (ct != null) {
                    // Wait for checkpointer thread to finish.
                    try {
                        ct.join();
                    } catch (InterruptedException e) {
                        // Ignore.
                    }
                }
            }
        }

        try {
            mCheckpointer = null;

            CommitLock lock = mCommitLock;

            if (mOpenTrees != null) {
                // Clear out open trees with commit lock held, to prevent any trees from being
                // opened again. Any attempt to open a tree must acquire the commit lock and
                // then check if the database is closed.
                final ArrayList<_TreeRef> trees;
                if (lock != null) {
                    lock.acquireExclusive();
                }
                try {
                    mOpenTreesLatch.acquireExclusive();
                    try {
                        trees = new ArrayList<>(mOpenTreesById.size());

                        mOpenTreesById.traverse(entry -> {
                            trees.add(entry.value);
                            return true;
                        });

                        mOpenTrees.clear();
                    } finally {
                        mOpenTreesLatch.releaseExclusive();
                    }
                } finally {
                    if (lock != null) {
                        lock.releaseExclusive();
                    }
                }

                for (_TreeRef ref : trees) {
                    _Tree tree = ref.get();
                    if (tree != null) {
                        tree.forceClose();
                    }
                }

                _FragmentedTrash trash = mFragmentedTrash;
                if (trash != null) {
                    mFragmentedTrash = null;
                    trash.mTrash.forceClose();
                }

                if (mCursorRegistry != null) {
                    mCursorRegistry.forceClose();
                }

                if (mRegistryKeyMap != null) {
                    mRegistryKeyMap.forceClose();
                }

                if (mRegistry != null) {
                    mRegistry.forceClose();
                }
            }

            if (lock != null) {
                lock.acquireExclusive();
            }
            try {
                if (mSorterExecutor != null) {
                    mSorterExecutor.shutdown();
                    mSorterExecutor = null;
                }

                if (mNodeContexts != null) {
                    for (_NodeContext context : mNodeContexts) {
                        if (context != null) {
                            context.delete();
                        }
                    }
                }

                if (mTxnContexts != null) {
                    for (_TransactionContext txnContext : mTxnContexts) {
                        if (txnContext != null) {
                            txnContext.deleteUndoLogs();
                        }
                    }
                }

                nodeMapDeleteAll();

                redoClose(RedoOps.OP_CLOSE, cause);

                IOException ex = null;
                ex = closeQuietly(ex, mPageDb, cause);
                ex = closeQuietly(ex, mTempFileManager, cause);

                if (shutdown && mBaseFile != null && !mReadOnly) {
                    deleteRedoLogFiles();
                    new File(mBaseFile.getPath() + INFO_FILE_SUFFIX).delete();
                    ex = closeQuietly(ex, mLockFile, cause);
                    new File(mBaseFile.getPath() + LOCK_FILE_SUFFIX).delete();
                } else {
                    ex = closeQuietly(ex, mLockFile, cause);
                }

                if (mLockManager != null) {
                    mLockManager.close();
                }

                if (ex != null) {
                    throw ex;
                }
            } finally {
                if (lock != null) {
                    lock.releaseExclusive();
                }
            }
        } finally {
            if (mPageDb != null) {
                mPageDb.delete();
            }
            if (mSparePagePool != null) {
                mSparePagePool.delete();
            }
            deleteCommitHeader();
            p_arenaDelete(mArena);
        }
    }

    private void deleteCommitHeader() {
        /*P*/ // [
        // mCommitHeader = null;
        /*P*/ // |
        p_delete(cCommitHeaderUpdater.getAndSet(this, p_null()));
        /*P*/ // ]
    }

    boolean isClosed() {
        return mClosed != 0;
    }

    void checkClosed() throws DatabaseException {
        if (isClosed()) {
            String message = "Closed";
            Throwable cause = mClosedCause;
            if (cause != null) {
                message += "; " + rootCause(cause);
            }
            throw new DatabaseException(message, cause);
        }
    }

    Throwable closedCause() {
        return mClosedCause;
    }

    void treeClosed(_Tree tree) {
        mOpenTreesLatch.acquireExclusive();
        try {
            _TreeRef ref = mOpenTreesById.getValue(tree.mId);
            if (ref != null && ref.get() == tree) {
                ref.clear();
                if (tree.mName != null) {
                    mOpenTrees.remove(tree.mName);
                }
                mOpenTreesById.remove(tree.mId);
            }
        } finally {
            mOpenTreesLatch.releaseExclusive();
        }
    }

    /**
     * @return false if already in the trash
     */
    private boolean moveToTrash(long treeId, byte[] treeIdBytes) throws IOException {
        final byte[] idKey = newKey(KEY_TYPE_INDEX_ID, treeIdBytes);
        final byte[] trashIdKey = newKey(KEY_TYPE_TRASH_ID, treeIdBytes);

        final _LocalTransaction txn = newAlwaysRedoTransaction();

        try {
            txn.lockTimeout(-1, null);

            if (mRegistryKeyMap.load(txn, trashIdKey) != null) {
                // Already in the trash.
                return false;
            }

            byte[] treeName = mRegistryKeyMap.exchange(txn, idKey, null);

            if (treeName == null) {
                // A trash entry with just a zero indicates that the name is null.
                mRegistryKeyMap.store(txn, trashIdKey, new byte[1]);
            } else {
                byte[] nameKey = newKey(KEY_TYPE_INDEX_NAME, treeName);
                mRegistryKeyMap.remove(txn, nameKey, treeIdBytes);
                // Tag the trash entry to indicate that name is non-null. Note that nameKey
                // instance is modified directly.
                nameKey[0] = 1;
                mRegistryKeyMap.store(txn, trashIdKey, nameKey);
            }

            if (txn.mRedo != null) {
                // Note: No additional operations can appear after OP_DELETE_INDEX. When a
                // replica reads this operation it immediately commits the transaction in order
                // for the deletion task to be started immediately. The redo log still contains
                // a commit operation, which is redundant and harmless.

                txn.durabilityMode(mDurabilityMode.alwaysRedo());

                long commitPos;
                CommitLock.Shared shared = mCommitLock.acquireShared();
                try {
                    txn.check();
                    commitPos = txn.mContext.redoDeleteIndexCommitFinal
                        (txn.mRedo, txn.txnId(), treeId, txn.durabilityMode());
                } finally {
                    shared.release();
                }

                if (commitPos != 0) {
                    // Must wait for durability confirmation before performing actions below
                    // which cannot be easily rolled back. No global latches or locks are held
                    // while waiting.
                    txn.mRedo.txnCommitSync(txn, commitPos);
                }
            }

            txn.commit();
        } catch (Throwable e) {
            DatabaseException.rethrowIfRecoverable(e);
            throw closeOnFailure(this, e);
        } finally {
            txn.reset();
        }

        return true;
    }

    /**
     * Must be called after all entries in the tree have been deleted and tree is closed.
     */
    void removeFromTrash(_Tree tree, _Node root) throws IOException {
        byte[] trashIdKey = newKey(KEY_TYPE_TRASH_ID, tree.mIdBytes);

        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            if (root != null) {
                root.acquireExclusive();
                if (root.mPage == p_closedTreePage()) {
                    // Database has been closed.
                    root.releaseExclusive();
                    return;
                }
                deleteNode(root);
            }
            mRegistryKeyMap.delete(Transaction.BOGUS, trashIdKey);
            mRegistry.delete(Transaction.BOGUS, tree.mIdBytes);
        } catch (Throwable e) {
            throw closeOnFailure(this, e);
        } finally {
            shared.release();
        }
    }

    /**
     * Removes all references to a temporary tree which was grafted to another one. Caller must
     * hold shared commit lock.
     */
    void removeGraftedTempTree(_Tree tree) throws IOException {
        try {
            mOpenTreesLatch.acquireExclusive();
            try {
                _TreeRef ref = mOpenTreesById.removeValue(tree.mId);
                if (ref != null && ref.get() == tree) {
                    ref.clear();
                }
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }
            byte[] trashIdKey = newKey(KEY_TYPE_TRASH_ID, tree.mIdBytes);
            mRegistryKeyMap.delete(Transaction.BOGUS, trashIdKey);
            mRegistry.delete(Transaction.BOGUS, tree.mIdBytes);
        } catch (Throwable e) {
            throw closeOnFailure(this, e);
        }
    }

    /**
     * Should be called before attempting to register a cursor, in case an exception is thrown.
     */
    _Tree openCursorRegistry() throws IOException {
        _Tree cursorRegistry = mCursorRegistry;
        if (cursorRegistry == null) {
            mOpenTreesLatch.acquireExclusive();
            try {
                if ((cursorRegistry = mCursorRegistry) == null) {
                    mCursorRegistry = cursorRegistry =
                        openInternalTree(_Tree.CURSOR_REGISTRY_ID, true);
                }
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }
        }

        return cursorRegistry;
    }

    /**
     * Should be called after the cursor id has been assigned, with the commit lock held.
     */
    void registerCursor(_Tree cursorRegistry, _TreeCursor cursor) throws IOException {
        try {
            byte[] cursorIdBytes = new byte[8];
            encodeLongBE(cursorIdBytes, 0, cursor.mCursorId);
            cursorRegistry.store(Transaction.BOGUS, cursorIdBytes, cursor.mTree.mIdBytes);
        } catch (Throwable e) {
            try {
                cursor.unregister();
            } catch (Throwable e2) {
                suppress(e, e2);
            }
            throw e;
        }
    }

    void unregisterCursor(_TreeCursor cursor) {
        try {
            byte[] cursorIdBytes = new byte[8];
            encodeLongBE(cursorIdBytes, 0, cursor.mCursorId);
            openCursorRegistry().store(Transaction.BOGUS, cursorIdBytes, null);
            cursor.mCursorId = 0;
        } catch (Throwable e) {
            // Database is borked, cleanup later.
        }
    }

    /**
     * @param treeId pass zero if unknown or not applicable
     * @param rootId pass zero to create
     * @return unlatched and unevictable root node
     */
    private _Node loadTreeRoot(final long treeId, final long rootId) throws IOException {
        if (rootId == 0) {
            // Pass tree identifier to spread allocations around.
            _Node rootNode = allocLatchedNode(treeId, _NodeContext.MODE_UNEVICTABLE);

            try {
                /*P*/ // [
                // rootNode.asEmptyRoot();
                /*P*/ // |
                if (mFullyMapped) {
                    rootNode.mPage = p_nonTreePage(); // always an empty leaf node
                    rootNode.mId = 0;
                    rootNode.mCachedState = CACHED_CLEAN;
                } else {
                    rootNode.asEmptyRoot();
                }
                /*P*/ // ]
                return rootNode;
            } finally {
                rootNode.releaseExclusive();
            }
        } else {
            // Check if root node is still around after tree was closed.
            _Node rootNode = nodeMapGetAndRemove(rootId);

            if (rootNode != null) {
                try {
                    rootNode.makeUnevictable();
                    return rootNode;
                } finally {
                    rootNode.releaseExclusive();
                }
            }

            rootNode = allocLatchedNode(rootId, _NodeContext.MODE_UNEVICTABLE);

            try {
                try {
                    rootNode.read(this, rootId);
                } finally {
                    rootNode.releaseExclusive();
                }
                return rootNode;
            } catch (Throwable e) {
                rootNode.makeEvictableNow();
                throw e;
            }
        }
    }

    /**
     * Loads the root registry node, or creates one if store is new. Root node
     * is not eligible for eviction.
     */
    private _Node loadRegistryRoot(DatabaseConfig config, byte[] header) throws IOException {
        int version = decodeIntLE(header, I_ENCODING_VERSION);

        if (config.mDebugOpen != null) {
            mEventListener.notify(EventType.DEBUG, "ENCODING_VERSION: %1$d", version);
        }

        long rootId;
        if (version == 0) {
            rootId = 0;
            // No registry; clearly nothing has been checkpointed.
            mInitialReadState = CACHED_DIRTY_0;
        } else {
            if (version != ENCODING_VERSION) {
                throw new CorruptDatabaseException("Unknown encoding version: " + version);
            }

            long replEncoding = decodeLongLE(header, I_REPL_ENCODING);

            if (config.mDebugOpen != null) {
                mEventListener.notify(EventType.DEBUG, "REPL_ENCODING: %1$d", replEncoding);
            }

            ReplicationManager rm = config.mReplManager;

            if (rm == null) {
                if (replEncoding != 0) {
                    throw new DatabaseException
                        ("Database must be configured with a replication manager, " +
                         "identified by: " + replEncoding);
                }
            } else {
                if (replEncoding == 0) {
                    throw new DatabaseException
                        ("Database was created initially without a replication manager");
                }
                long expectedReplEncoding = rm.encoding();
                if (replEncoding != expectedReplEncoding) {
                    throw new DatabaseException
                        ("Database was created initially with a different replication manager, " +
                         "identified by: " + replEncoding);
                }
            }

            rootId = decodeLongLE(header, I_ROOT_PAGE_ID);

            if (config.mDebugOpen != null) {
                mEventListener.notify(EventType.DEBUG, "ROOT_PAGE_ID: %1$d", rootId);
            }
        }

        return loadTreeRoot(0, rootId);
    }

    private _Tree openInternalTree(long treeId, boolean create) throws IOException {
        return openInternalTree(treeId, create, null);
    }

    private _Tree openInternalTree(long treeId, boolean create, DatabaseConfig config)
        throws IOException
    {
        CommitLock.Shared shared = mCommitLock.acquireShared();
        try {
            checkClosed();

            byte[] treeIdBytes = new byte[8];
            encodeLongBE(treeIdBytes, 0, treeId);
            byte[] rootIdBytes = mRegistry.load(Transaction.BOGUS, treeIdBytes);
            long rootId;
            if (rootIdBytes != null) {
                rootId = decodeLongLE(rootIdBytes, 0);
            } else {
                if (!create) {
                    return null;
                }
                rootId = 0;
            }

            _Node root = loadTreeRoot(treeId, rootId);

            // Cannot call newTreeInstance because mRedoWriter isn't set yet.
            if (config != null && config.mReplManager != null) {
                return new _TxnTree(this, treeId, treeIdBytes, root);
            }

            return newTreeInstance(treeId, treeIdBytes, null, root);
        } finally {
            shared.release();
        }
    }

    /**
     * @param name required (cannot be null)
     */
    private _Tree openTree(byte[] name, boolean create) throws IOException {
        return openTree(null, null, name, create);
    }

    /**
     * @param findTxn optional
     * @param treeIdBytes optional
     * @param name required (cannot be null)
     */
    private _Tree openTree(Transaction findTxn, byte[] treeIdBytes, byte[] name, boolean create)
        throws IOException
    {
        _Tree tree = quickFindIndex(name);
        if (tree == null) {
            CommitLock.Shared shared = mCommitLock.acquireShared();
            try {
                tree = doOpenTree(findTxn, treeIdBytes, name, create);
            } finally {
                shared.release();
            }
        }
        return tree;
    }

    /**
     * Caller must hold commit lock.
     *
     * @param findTxn optional
     * @param treeIdBytes optional
     * @param name required (cannot be null)
     */
    private _Tree doOpenTree(Transaction findTxn, byte[] treeIdBytes, byte[] name, boolean create)
        throws IOException
    {
        checkClosed();

        // Cleanup before opening more trees.
        cleanupUnreferencedTrees();

        byte[] nameKey = newKey(KEY_TYPE_INDEX_NAME, name);

        if (treeIdBytes == null) {
            treeIdBytes = mRegistryKeyMap.load(findTxn, nameKey);
        }

        long treeId;
        // Is non-null if tree was created.
        byte[] idKey;

        if (treeIdBytes != null) {
            // _Tree already exists.
            idKey = null;
            treeId = decodeLongBE(treeIdBytes, 0);
        } else if (!create) {
            return null;
        } else create: {
            // Transactional find supported only for opens that do not create.
            if (findTxn != null) {
                throw new AssertionError();
            }

            Transaction createTxn = null;

            mOpenTreesLatch.acquireExclusive();
            try {
                treeIdBytes = mRegistryKeyMap.load(null, nameKey);
                if (treeIdBytes != null) {
                    // Another thread created it.
                    idKey = null;
                    treeId = decodeLongBE(treeIdBytes, 0);
                    break create;
                }

                treeIdBytes = new byte[8];

                // Non-transactional operations are critical, in that any failure is treated as
                // non-recoverable.
                boolean critical = true;
                try {
                    do {
                        critical = false;
                        treeId = nextTreeId(false);
                        encodeLongBE(treeIdBytes, 0, treeId);
                        critical = true;
                    } while (!mRegistry.insert(Transaction.BOGUS, treeIdBytes, EMPTY_BYTES));

                    critical = false;

                    try {
                        idKey = newKey(KEY_TYPE_INDEX_ID, treeIdBytes);

                        if (mRedoWriter instanceof _ReplRedoController) {
                            // Confirmation is required when replicated.
                            createTxn = newTransaction(DurabilityMode.SYNC);
                        } else {
                            createTxn = newAlwaysRedoTransaction();
                        }

                        createTxn.lockTimeout(-1, null);

                        // Insert order is important for the indexById method to work reliably.
                        if (!mRegistryKeyMap.insert(createTxn, idKey, name)) {
                            throw new DatabaseException("Unable to insert index id");
                        }
                        if (!mRegistryKeyMap.insert(createTxn, nameKey, treeIdBytes)) {
                            throw new DatabaseException("Unable to insert index name");
                        }
                    } catch (Throwable e) {
                        critical = true;
                        try {
                            if (createTxn != null) {
                                createTxn.reset();
                            }
                            mRegistry.delete(Transaction.BOGUS, treeIdBytes);
                            critical = false;
                        } catch (Throwable e2) {
                            Utils.suppress(e, e2);
                        }
                        throw e;
                    }
                } catch (Throwable e) {
                    if (!critical) {
                        DatabaseException.rethrowIfRecoverable(e);
                    }
                    throw closeOnFailure(this, e);
                }
            } finally {
                // Release to allow opening other indexes while blocked on commit.
                mOpenTreesLatch.releaseExclusive();
            }

            if (createTxn != null) {
                try {
                    createTxn.commit();
                } catch (Throwable e) {
                    try {
                        createTxn.reset();
                        mRegistry.delete(Transaction.BOGUS, treeIdBytes);
                    } catch (Throwable e2) {
                        Utils.suppress(e, e2);
                        throw closeOnFailure(this, e);
                    }
                    DatabaseException.rethrowIfRecoverable(e);
                    throw closeOnFailure(this, e);
                }
            }
        }

        // Use a transaction to ensure that only one thread loads the requested tree. Nothing
        // is written into it.
        Transaction txn = newNoRedoTransaction();
        try {
            txn.lockTimeout(-1, null);

            // Pass the transaction to acquire the lock.
            byte[] rootIdBytes = mRegistry.load(txn, treeIdBytes);

            _Tree tree = quickFindIndex(name);
            if (tree != null) {
                // Another thread got the lock first and loaded the tree.
                return tree;
            }

            long rootId = (rootIdBytes == null || rootIdBytes.length == 0) ? 0
                : decodeLongLE(rootIdBytes, 0);

            _Node root = loadTreeRoot(treeId, rootId);

            tree = newTreeInstance(treeId, treeIdBytes, name, root);
            _TreeRef treeRef = new _TreeRef(tree, mOpenTreesRefQueue);

            mOpenTreesLatch.acquireExclusive();
            try {
                mOpenTrees.put(name, treeRef);
                mOpenTreesById.insert(treeId).value = treeRef;
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }

            return tree;
        } catch (Throwable e) {
            if (idKey != null) {
                // Rollback create of new tree.
                try {
                    mRegistryKeyMap.delete(null, idKey);
                    mRegistryKeyMap.delete(null, nameKey);
                    mRegistry.delete(Transaction.BOGUS, treeIdBytes);
                } catch (Throwable e2) {
                    // Ignore.
                }
            }
            throw e;
        } finally {
            txn.reset();
        }
    }

    private _Tree newTreeInstance(long id, byte[] idBytes, byte[] name, _Node root) {
        _Tree tree;
        if (mRedoWriter instanceof _ReplRedoWriter) {
            // Always need an explcit transaction when using auto-commit, to ensure that
            // rollback is possible.
            tree = new _TxnTree(this, id, idBytes, root);
        } else {
            tree = new _Tree(this, id, idBytes, root);
        }
        tree.mName = name;
        return tree;
    }

    private long nextTreeId(boolean temporary) throws IOException {
        // By generating identifiers from a 64-bit sequence, it's effectively
        // impossible for them to get re-used after trees are deleted.

        Transaction txn;
        if (temporary) {
            txn = newNoRedoTransaction();
        } else {
            txn = newAlwaysRedoTransaction();
        }

        try {
            txn.lockTimeout(-1, null);

            // _Tree id mask, to make the identifiers less predictable and
            // non-compatible with other database instances.
            long treeIdMask;
            {
                byte[] key = {KEY_TYPE_TREE_ID_MASK};
                byte[] treeIdMaskBytes = mRegistryKeyMap.load(txn, key);

                if (treeIdMaskBytes == null) {
                    treeIdMaskBytes = new byte[8];
                    ThreadLocalRandom.current().nextBytes(treeIdMaskBytes);
                    mRegistryKeyMap.store(txn, key, treeIdMaskBytes);
                }

                treeIdMask = decodeLongLE(treeIdMaskBytes, 0);
            }

            byte[] key = {KEY_TYPE_NEXT_TREE_ID};
            byte[] nextTreeIdBytes = mRegistryKeyMap.load(txn, key);

            if (nextTreeIdBytes == null) {
                nextTreeIdBytes = new byte[8];
            }
            long nextTreeId = decodeLongLE(nextTreeIdBytes, 0);

            if (temporary) {
                // Apply negative sequence, avoiding collisions.
                treeIdMask = ~treeIdMask;
            }

            long treeId;
            do {
                treeId = scramble((nextTreeId++) ^ treeIdMask);
            } while (_Tree.isInternal(treeId));

            encodeLongLE(nextTreeIdBytes, 0, nextTreeId);
            mRegistryKeyMap.store(txn, key, nextTreeIdBytes);
            txn.commit();

            return treeId;
        } finally {
            txn.reset();
        }
    }

    /**
     * @param name required (cannot be null)
     * @return null if not found
     */
    private _Tree quickFindIndex(byte[] name) throws IOException {
        _TreeRef treeRef;
        mOpenTreesLatch.acquireShared();
        try {
            treeRef = mOpenTrees.get(name);
            if (treeRef == null) {
                return null;
            }
            _Tree tree = treeRef.get();
            if (tree != null) {
                return tree;
            }
        } finally {
            mOpenTreesLatch.releaseShared();
        }

        // Ensure that root node of cleared tree reference is available in the node map before
        // potentially replacing it. Weak references are cleared before they are enqueued, and
        // so polling the queue does not guarantee node eviction. Process the tree directly.
        cleanupUnreferencedTree(treeRef);

        return null;
    }

    /**
     * _Tree instances retain a reference to an unevictable root node. If tree is no longer in
     * use, allow it to be evicted. Method cannot be called while a checkpoint is in progress.
     */
    private void cleanupUnreferencedTrees() throws IOException {
        final ReferenceQueue<_Tree> queue = mOpenTreesRefQueue;
        if (queue == null) {
            return;
        }
        try {
            while (true) {
                Reference<? extends _Tree> ref = queue.poll();
                if (ref == null) {
                    break;
                }
                if (ref instanceof _TreeRef) {
                    cleanupUnreferencedTree((_TreeRef) ref);
                }
            }
        } catch (Exception e) {
            if (!isClosed()) {
                throw e;
            }
        }
    }

    private void cleanupUnreferencedTree(_TreeRef ref) throws IOException {
        _Node root = ref.mRoot;
        root.acquireShared();
        try {
            mOpenTreesLatch.acquireExclusive();
            try {
                LHashTable.ObjEntry<_TreeRef> entry = mOpenTreesById.get(ref.mId);
                if (entry == null || entry.value != ref) {
                    return;
                }
                if (ref.mName != null) {
                    mOpenTrees.remove(ref.mName);
                }
                mOpenTreesById.remove(ref.mId);
                root.makeEvictableNow();
                if (root.mId != 0) {
                    nodeMapPut(root);
                }
            } finally {
                mOpenTreesLatch.releaseExclusive();
            }
        } finally {
            root.releaseShared();
        }
    }

    private static byte[] newKey(byte type, byte[] payload) {
        byte[] key = new byte[1 + payload.length];
        key[0] = type;
        arraycopy(payload, 0, key, 1, payload.length);
        return key;
    }

    /**
     * Returns the fixed size of all pages in the store, in bytes.
     */
    int pageSize() {
        return mPageSize;
    }

    private int pageSize(long page) {
        /*P*/ // [
        // return page.length;
        /*P*/ // |
        return mPageSize;
        /*P*/ // ]
    }

    /**
     * Access the commit lock, which prevents commits while held shared. In general, it should
     * be acquired before any node latches, but postponing acquisition reduces the total time
     * held. Checkpoints don't have to wait as long for the exclusive commit lock. Because node
     * latching first isn't the canonical ordering, acquiring the shared commit lock later must
     * be prepared to abort. Try to acquire first, and if it fails, release the node latch and
     * do over.
     */
    CommitLock commitLock() {
        return mCommitLock;
    }

    /**
     * @return shared latched node if found; null if not found
     */
    _Node nodeMapGetShared(long nodeId) {
        int hash = Long.hashCode(nodeId);
        while (true) {
            _Node node = nodeMapGet(nodeId, hash);
            if (node == null) {
                return null;
            }
            node.acquireShared();
            if (nodeId == node.mId) {
                return node;
            }
            node.releaseShared();
        }
    }

    /**
     * @return exclusively latched node if found; null if not found
     */
    _Node nodeMapGetExclusive(long nodeId) {
        int hash = Long.hashCode(nodeId);
        while (true) {
            _Node node = nodeMapGet(nodeId, hash);
            if (node == null) {
                return null;
            }
            node.acquireExclusive();
            if (nodeId == node.mId) {
                return node;
            }
            node.releaseExclusive();
        }
    }

    /**
     * Returns unconfirmed node if found. Caller must latch and confirm that node identifier
     * matches, in case an eviction snuck in.
     */
    _Node nodeMapGet(final long nodeId) {
        return nodeMapGet(nodeId, Long.hashCode(nodeId));
    }

    /**
     * Returns unconfirmed node if found. Caller must latch and confirm that node identifier
     * matches, in case an eviction snuck in.
     */
    _Node nodeMapGet(final long nodeId, final int hash) {
        // Quick check without acquiring a partition latch.

        final _Node[] table = mNodeMapTable;
        _Node node = table[hash & (table.length - 1)];
        if (node != null) {
            // Limit scan of collision chain in case a temporary infinite loop is observed.
            int limit = 100;
            do {
                if (node.mId == nodeId) {
                    return node;
                }
            } while ((node = node.mNodeMapNext) != null && --limit != 0);
        }

        // Again with shared partition latch held.

        final Latch[] latches = mNodeMapLatches;
        final Latch latch = latches[hash & (latches.length - 1)];
        latch.acquireShared();

        node = table[hash & (table.length - 1)];
        while (node != null) {
            if (node.mId == nodeId) {
                latch.releaseShared();
                return node;
            }
            node = node.mNodeMapNext;
        }

        latch.releaseShared();
        return null;
    }

    /**
     * Put a node into the map, but caller must confirm that node is not already present.
     */
    void nodeMapPut(final _Node node) {
        nodeMapPut(node, Long.hashCode(node.mId));
    }

    /**
     * Put a node into the map, but caller must confirm that node is not already present.
     */
    void nodeMapPut(final _Node node, final int hash) {
        final Latch[] latches = mNodeMapLatches;
        final Latch latch = latches[hash & (latches.length - 1)];
        latch.acquireExclusive();

        final _Node[] table = mNodeMapTable;
        final int index = hash & (table.length - 1);
        _Node e = table[index];
        while (e != null) {
            if (e == node) {
                latch.releaseExclusive();
                return;
            }
            if (e.mId == node.mId) {
                latch.releaseExclusive();
                throw new AssertionError("Already in NodeMap: " + node + ", " + e + ", " + hash);
            }
            e = e.mNodeMapNext;
        }

        node.mNodeMapNext = table[index];
        table[index] = node;

        latch.releaseExclusive();
    }

    /**
     * Returns unconfirmed node if an existing node is found. Caller must latch and confirm
     * that node identifier matches, in case an eviction snuck in.
     *
     * @return null if node was inserted, existing node otherwise
     */
    _Node nodeMapPutIfAbsent(final _Node node) {
        final int hash = Long.hashCode(node.mId);
        final Latch[] latches = mNodeMapLatches;
        final Latch latch = latches[hash & (latches.length - 1)];
        latch.acquireExclusive();

        final _Node[] table = mNodeMapTable;
        final int index = hash & (table.length - 1);
        _Node e = table[index];
        while (e != null) {
            if (e.mId == node.mId) {
                latch.releaseExclusive();
                return e;
            }
            e = e.mNodeMapNext;
        }

        node.mNodeMapNext = table[index];
        table[index] = node;

        latch.releaseExclusive();
        return null;
    }

    /**
     * Replace a node which must be in the map already. Old and new node MUST have the same id.
     */
    void nodeMapReplace(final _Node oldNode, final _Node newNode) {
        final int hash = Long.hashCode(oldNode.mId);
        final Latch[] latches = mNodeMapLatches;
        final Latch latch = latches[hash & (latches.length - 1)];
        latch.acquireExclusive();

        newNode.mNodeMapNext = oldNode.mNodeMapNext;

        final _Node[] table = mNodeMapTable;
        final int index = hash & (table.length - 1);
        _Node e = table[index];
        if (e == oldNode) {
            table[index] = newNode;
        } else while (e != null) {
            _Node next = e.mNodeMapNext;
            if (next == oldNode) {
                e.mNodeMapNext = newNode;
                break;
            }
            e = next;
        }

        oldNode.mNodeMapNext = null;

        latch.releaseExclusive();
    }

    boolean nodeMapRemove(final _Node node) {
        return nodeMapRemove(node, Long.hashCode(node.mId));
    }

    boolean nodeMapRemove(final _Node node, final int hash) {
        boolean found = false;

        final Latch[] latches = mNodeMapLatches;
        final Latch latch = latches[hash & (latches.length - 1)];
        latch.acquireExclusive();

        final _Node[] table = mNodeMapTable;
        final int index = hash & (table.length - 1);
        _Node e = table[index];
        if (e == node) {
            found = true;
            table[index] = e.mNodeMapNext;
        } else while (e != null) {
            _Node next = e.mNodeMapNext;
            if (next == node) {
                found = true;
                e.mNodeMapNext = next.mNodeMapNext;
                break;
            }
            e = next;
        }

        node.mNodeMapNext = null;

        latch.releaseExclusive();

        return found;
    }

    /**
     * Returns or loads the fragment node with the given id. If loaded, node is put in the cache.
     *
     * @return node with shared latch held
     */
    _Node nodeMapLoadFragment(long nodeId) throws IOException {
        _Node node = nodeMapGetShared(nodeId);

        if (node != null) {
            node.used(ThreadLocalRandom.current());
            return node;
        }

        node = allocLatchedNode(nodeId);
        node.mId = nodeId;

        // node is currently exclusively locked. Insert it into the node map so that no other
        // thread tries to read it at the same time. If another thread sees it at this point
        // (before it is actually read), until the node is read, that thread will block trying
        // to get a shared lock.
        while (true) {
            _Node existing = nodeMapPutIfAbsent(node);
            if (existing == null) {
                break;
            }

            // Was already loaded, or is currently being loaded.
            existing.acquireShared();
            if (nodeId == existing.mId) {
                // The item is already loaded. Throw away the node this thread was trying to
                // allocate.
                //
                // Even though node is not currently in the node map, it could have been in
                // there then got recycled. Other thread may still have a reference to it from
                // when it was in the node map. So its id needs to be invalidated.
                node.mId = 0;
                // This releases the exclusive latch and makes the node immediately usable for
                // new allocations.
                node.unused();
                return existing;
            }
            existing.releaseShared();
        }

        try {
            /*P*/ // [
            // node.type(TYPE_FRAGMENT);
            /*P*/ // ]
            readNode(node, nodeId);
        } catch (Throwable t) {
            // Something went wrong reading the node. Remove the node from the map, now that
            // it definitely won't get read.
            nodeMapRemove(node);
            node.mId = 0;
            node.releaseExclusive();
            throw t;
        }
        node.downgrade();

        return node;
    }

    /**
     * Returns or loads the fragment node with the given id. If loaded, node is put in the
     * cache. Method is intended for obtaining nodes to write into.
     *
     * @param read true if node should be fully read if it needed to be loaded
     * @return node with exclusive latch held
     */
    _Node nodeMapLoadFragmentExclusive(long nodeId, boolean read) throws IOException {
        // Very similar to the nodeMapLoadFragment method. It has comments which explains
        // what's going on here. No point in duplicating that as well.

        _Node node = nodeMapGetExclusive(nodeId);

        if (node != null) {
            node.used(ThreadLocalRandom.current());
            return node;
        }

        node = allocLatchedNode(nodeId);
        node.mId = nodeId;

        while (true) {
            _Node existing = nodeMapPutIfAbsent(node);
            if (existing == null) {
                break;
            }
            existing.acquireExclusive();
            if (nodeId == existing.mId) {
                node.mId = 0;
                node.unused();
                return existing;
            }
            existing.releaseExclusive();
        }

        try {
            /*P*/ // [
            // node.type(TYPE_FRAGMENT);
            /*P*/ // ]
            if (read) {
                readNode(node, nodeId);
            }
        } catch (Throwable t) {
            nodeMapRemove(node);
            node.mId = 0;
            node.releaseExclusive();
            throw t;
        }

        return node;
    }

    /**
     * @return exclusively latched node if found; null if not found
     */
    _Node nodeMapGetAndRemove(long nodeId) {
        _Node node = nodeMapGetExclusive(nodeId);
        if (node != null) {
            nodeMapRemove(node);
        }
        return node;
    }

    /**
     * Remove and delete nodes from map, as part of close sequence.
     */
    void nodeMapDeleteAll() {
        start: while (true) {
            for (Latch latch : mNodeMapLatches) {
                latch.acquireExclusive();
            }

            try {
                for (int i=mNodeMapTable.length; --i>=0; ) {
                    _Node e = mNodeMapTable[i];
                    if (e != null) {
                        if (!e.tryAcquireExclusive()) {
                            // Deadlock prevention.
                            continue start;
                        }
                        try {
                            e.doDelete(this);
                        } finally {
                            e.releaseExclusive();
                        }
                        _Node next;
                        while ((next = e.mNodeMapNext) != null) {
                            e.mNodeMapNext = null;
                            e = next;
                        }
                        mNodeMapTable[i] = null;
                    }
                }
            } finally {
                for (Latch latch : mNodeMapLatches) {
                    latch.releaseExclusive();
                }
            }

            return;
        }
    }

    /**
     * With parent held shared, returns child with shared latch held, releasing the parent
     * latch. If an exception is thrown, parent and child latches are always released.
     *
     * @return child node, possibly split
     */
    final _Node latchToChild(_Node parent, int childPos) throws IOException {
        return latchChild(parent, childPos, _Node.OPTION_PARENT_RELEASE_SHARED);
    }

    /**
     * With parent held shared, returns child with shared latch held, retaining the parent
     * latch. If an exception is thrown, parent and child latches are always released.
     *
     * @return child node, possibly split
     */
    final _Node latchChildRetainParent(_Node parent, int childPos) throws IOException {
        return latchChild(parent, childPos, 0);
    }

    /**
     * With parent held shared, returns child with shared latch held. If an exception is
     * thrown, parent and child latches are always released.
     *
     * @param option _Node.OPTION_PARENT_RELEASE_SHARED or 0 to retain latch
     * @return child node, possibly split
     */
    final _Node latchChild(_Node parent, int childPos, int option) throws IOException {
        long childId = parent.retrieveChildRefId(childPos);
        _Node childNode = nodeMapGetShared(childId);

        tryFind: if (childNode != null) {
            checkChild: {
                evictChild: if (childNode.mCachedState != _Node.CACHED_CLEAN
                                && parent.mCachedState == _Node.CACHED_CLEAN
                                // Must be a valid parent -- not a stub from _Node.rootDelete.
                                && parent.mId > 1)
                {
                    // Parent was evicted before child. Evict child now and mark as clean. If
                    // this isn't done, the notSplitDirty method will short-circuit and not
                    // ensure that all the parent nodes are dirty. The splitting and merging
                    // code assumes that all nodes referenced by the cursor are dirty. The
                    // short-circuit check could be skipped, but then every change would
                    // require a full latch up the tree. Another option is to remark the parent
                    // as dirty, but this is dodgy and also requires a full latch up the tree.
                    // Parent-before-child eviction is infrequent, and so simple is better.

                    if (!childNode.tryUpgrade()) {
                        childNode.releaseShared();
                        childNode = nodeMapGetExclusive(childId);
                        if (childNode == null) {
                            break tryFind;
                        }
                        if (childNode.mCachedState == _Node.CACHED_CLEAN) {
                            // Child state which was checked earlier changed when its latch was
                            // released, and now it shoudn't be evicted.
                            childNode.downgrade();
                            break evictChild;
                        }
                    }

                    if (option == _Node.OPTION_PARENT_RELEASE_SHARED) {
                        parent.releaseShared();
                    }

                    try {
                        childNode.write(mPageDb);
                    } catch (Throwable e) {
                        childNode.releaseExclusive();
                        if (option == 0) {
                            // Release due to exception.
                            parent.releaseShared();
                        }
                        throw e;
                    }

                    childNode.mCachedState = _Node.CACHED_CLEAN;
                    childNode.downgrade();
                    break checkChild;
                }

                if (option == _Node.OPTION_PARENT_RELEASE_SHARED) {
                    parent.releaseShared();
                }
            }

            childNode.used(ThreadLocalRandom.current());
            return childNode;
        }

        return parent.loadChild(this, childId, option);
    }

    /**
     * Variant of latchChildRetainParent which uses exclusive latches. With parent held
     * exclusively, returns child with exclusive latch held, retaining the parent latch. If an
     * exception is thrown, parent and child latches are always released.
     *
     * @param required pass false to allow null to be returned when child isn't immediately
     * latchable; passing false still permits the child to be loaded if necessary
     * @return child node, possibly split
     */
    final _Node latchChildRetainParentEx(_Node parent, int childPos, boolean required)
        throws IOException
    {
        long childId = parent.retrieveChildRefId(childPos);

        _Node childNode;
        while (true) {
            childNode = nodeMapGet(childId);

            if (childNode != null) {
                if (required) {
                    childNode.acquireExclusive();
                } else if (!childNode.tryAcquireExclusive()) {
                    return null;
                }
                if (childId == childNode.mId) {
                    break;
                }
                childNode.releaseExclusive();
                continue;
            }

            return parent.loadChild(this, childId, _Node.OPTION_CHILD_ACQUIRE_EXCLUSIVE);
        }

        if (childNode.mCachedState != _Node.CACHED_CLEAN
            && parent.mCachedState == _Node.CACHED_CLEAN
            // Must be a valid parent -- not a stub from _Node.rootDelete.
            && parent.mId > 1)
        {
            // Parent was evicted before child. Evict child now and mark as clean. If
            // this isn't done, the notSplitDirty method will short-circuit and not
            // ensure that all the parent nodes are dirty. The splitting and merging
            // code assumes that all nodes referenced by the cursor are dirty. The
            // short-circuit check could be skipped, but then every change would
            // require a full latch up the tree. Another option is to remark the parent
            // as dirty, but this is dodgy and also requires a full latch up the tree.
            // Parent-before-child eviction is infrequent, and so simple is better.
            try {
                childNode.write(mPageDb);
            } catch (Throwable e) {
                childNode.releaseExclusive();
                // Release due to exception.
                parent.releaseExclusive();
                throw e;
            }
            childNode.mCachedState = _Node.CACHED_CLEAN;
        }

        childNode.used(ThreadLocalRandom.current());
        return childNode;
    }

    /**
     * Returns a new or recycled _Node instance, latched exclusively, with an undefined id and a
     * clean state.
     *
     * @param anyNodeId id of any node, for spreading allocations around
     */
    _Node allocLatchedNode(long anyNodeId) throws IOException {
        return allocLatchedNode(anyNodeId, 0);
    }

    /**
     * Returns a new or recycled _Node instance, latched exclusively, with an undefined id and a
     * clean state.
     *
     * @param anyNodeId id of any node, for spreading allocations around
     * @param mode MODE_UNEVICTABLE if allocated node cannot be automatically evicted
     */
    _Node allocLatchedNode(long anyNodeId, int mode) throws IOException {
        mode |= mPageDb.allocMode();

        _NodeContext[] contexts = mNodeContexts;
        int listIx = ((int) anyNodeId) & (contexts.length - 1);
        IOException fail = null;

        for (int trial = 1; trial <= 3; trial++) {
            for (int i=0; i<contexts.length; i++) {
                try {
                    _Node node = contexts[listIx].tryAllocLatchedNode(trial, mode);
                    if (node != null) {
                        return node;
                    }
                } catch (IOException e) {
                    if (fail == null) {
                        fail = e;
                    }
                }
                if (--listIx < 0) {
                    listIx = contexts.length - 1;
                }
            }

            checkClosed();

            CommitLock.Shared shared = mCommitLock.acquireShared();
            try {
                // Try to free up nodes from unreferenced trees.
                cleanupUnreferencedTrees();
            } finally {
                shared.release();
            }
        }

        if (fail == null && mPageDb.isDurable()) {
            throw new CacheExhaustedException();
        } else if (fail instanceof DatabaseFullException) {
            throw fail;
        } else {
            throw new DatabaseFullException(fail);
        }
    }

    /**
     * Returns a new or recycled _Node instance, latched exclusively and marked
     * dirty. Caller must hold commit lock.
     */
    _Node allocDirtyNode() throws IOException {
        return allocDirtyNode(0);
    }

    /**
     * Returns a new or recycled _Node instance, latched exclusively, marked
     * dirty and unevictable. Caller must hold commit lock.
     *
     * @param mode MODE_UNEVICTABLE if allocated node cannot be automatically evicted
     */
    _Node allocDirtyNode(int mode) throws IOException {
        _Node node = mPageDb.allocLatchedNode(this, mode);

        /*P*/ // [|
        if (mFullyMapped) {
            node.mPage = mPageDb.dirtyPage(node.mId);
        }
        /*P*/ // ]

        node.mContext.addDirty(node, mCommitState);
        return node;
    }

    /**
     * Returns a new or recycled _Node instance, latched exclusively and marked
     * dirty. Caller must hold commit lock.
     */
    _Node allocDirtyFragmentNode() throws IOException {
        _Node node = allocDirtyNode();
        nodeMapPut(node);
        /*P*/ // [
        // node.type(TYPE_FRAGMENT);
        /*P*/ // ]
        return node;
    }

    /**
     * Caller must hold commit lock and any latch on node.
     */
    boolean isMutable(_Node node) {
        return node.mCachedState == mCommitState && node.mId > 1;
    }

    /**
     * Caller must hold commit lock and any latch on node.
     */
    boolean shouldMarkDirty(_Node node) {
        return node.mCachedState != mCommitState && node.mId >= 0;
    }

    /**
     * Mark a tree node as dirty. Caller must hold commit lock and exclusive latch on
     * node. Method does nothing if node is already dirty. Latch is never released by this
     * method, even if an exception is thrown.
     *
     * @return true if just made dirty and id changed
     */
    boolean markDirty(_Tree tree, _Node node) throws IOException {
        if (node.mCachedState == mCommitState || node.mId < 0) {
            return false;
        } else {
            doMarkDirty(tree, node);
            return true;
        }
    }

    /**
     * Mark a fragment node as dirty. Caller must hold commit lock and exclusive latch on
     * node. Method does nothing if node is already dirty. Latch is never released by this
     * method, even if an exception is thrown.
     *
     * @return true if just made dirty and id changed
     */
    boolean markFragmentDirty(_Node node) throws IOException {
        if (node.mCachedState == mCommitState) {
            return false;
        } else {
            if (node.mCachedState != CACHED_CLEAN) {
                node.write(mPageDb);
            }

            long newId = mPageDb.allocPage();
            long oldId = node.mId;

            if (oldId != 0) {
                // Must be removed from map before page is deleted. It could be recycled too
                // soon, creating a NodeMap collision.
                boolean removed = nodeMapRemove(node, Long.hashCode(oldId));

                try {
                    // No need to force delete when dirtying. Caller is responsible for
                    // cleaning up.
                    mPageDb.deletePage(oldId, false);
                } catch (Throwable e) {
                    // Try to undo things.
                    if (removed) {
                        try {
                            nodeMapPut(node);
                        } catch (Throwable e2) {
                            Utils.suppress(e, e2);
                        }
                    }
                    try {
                        mPageDb.recyclePage(newId);
                    } catch (Throwable e2) {
                        // Panic.
                        Utils.suppress(e, e2);
                        close(e);
                    }
                    throw e;
                }
            }

            dirty(node, newId);
            nodeMapPut(node);
            return true;
        }
    }

    /**
     * Mark an unmapped node as dirty. Caller must hold commit lock and exclusive latch on
     * node. Method does nothing if node is already dirty. Latch is never released by this
     * method, even if an exception is thrown.
     */
    void markUnmappedDirty(_Node node) throws IOException {
        if (node.mCachedState != mCommitState) {
            node.write(mPageDb);

            long newId = mPageDb.allocPage();
            long oldId = node.mId;

            try {
                // No need to force delete when dirtying. Caller is responsible for cleaning up.
                mPageDb.deletePage(oldId, false);
            } catch (Throwable e) {
                try {
                    mPageDb.recyclePage(newId);
                } catch (Throwable e2) {
                    // Panic.
                    Utils.suppress(e, e2);
                    close(e);
                }
                throw e;
            }

            dirty(node, newId);
        }
    }

    /**
     * Caller must hold commit lock and exclusive latch on node. Method must
     * not be called if node is already dirty. Latch is never released by this
     * method, even if an exception is thrown.
     */
    void doMarkDirty(_Tree tree, _Node node) throws IOException {
        if (node.mCachedState != CACHED_CLEAN) {
            node.write(mPageDb);
        }

        long newId = mPageDb.allocPage();
        long oldId = node.mId;

        try {
            if (node == tree.mRoot) {
                storeTreeRootId(tree, newId);
            }
        } catch (Throwable e) {
            try {
                mPageDb.recyclePage(newId);
            } catch (Throwable e2) {
                // Panic.
                Utils.suppress(e, e2);
                close(e);
            }
            throw e;
        }

        if (oldId != 0) {
            // Must be removed from map before page is deleted. It could be recycled too soon,
            // creating a NodeMap collision.
            boolean removed = nodeMapRemove(node, Long.hashCode(oldId));

            try {
                // TODO: This can hang on I/O; release frame latch if deletePage would block?
                // Then allow thread to block without node latch held.
                // No need to force delete when dirtying. Caller is responsible for cleaning up.
                mPageDb.deletePage(oldId, false);
            } catch (Throwable e) {
                // Try to undo things.
                if (removed) {
                    try {
                        nodeMapPut(node);
                    } catch (Throwable e2) {
                        Utils.suppress(e, e2);
                    }
                }
                try {
                    if (node == tree.mRoot) {
                        storeTreeRootId(tree, oldId);
                    }
                    mPageDb.recyclePage(newId);
                } catch (Throwable e2) {
                    // Panic.
                    Utils.suppress(e, e2);
                    close(e);
                }
                throw e;
            }
        }

        dirty(node, newId);
        nodeMapPut(node);
    }

    private void storeTreeRootId(_Tree tree, long id) throws IOException {
        if (tree.mIdBytes != null) {
            byte[] encodedId = new byte[8];
            encodeLongLE(encodedId, 0, id);
            mRegistry.store(Transaction.BOGUS, tree.mIdBytes, encodedId);
        }
    }

    /**
     * Caller must hold commit lock and exclusive latch on node.
     */
    private void dirty(_Node node, long newId) throws IOException {
        /*P*/ // [|
        if (mFullyMapped) {
            if (node.mPage == p_nonTreePage()) {
                node.mPage = mPageDb.dirtyPage(newId);
                node.asEmptyRoot();
            } else if (node.mPage != p_closedTreePage()) {
                node.mPage = mPageDb.copyPage(node.mId, newId); // copy on write
            }
        }
        /*P*/ // ]

        node.mId = newId;
        node.mContext.addDirty(node, mCommitState);
    }

    /**
     * Remove the old node from the dirty list and swap in the new node. Caller must hold
     * commit lock and latched the old node. The cached state of the nodes is not altered.
     * Both nodes must belong to the same context.
     */
    void swapIfDirty(_Node oldNode, _Node newNode) {
        oldNode.mContext.swapIfDirty(oldNode, newNode);
    }

    /**
     * Caller must hold commit lock and exclusive latch on node. This method
     * should only be called for nodes whose existing data is not needed.
     */
    void redirty(_Node node) throws IOException {
        /*P*/ // [|
        if (mFullyMapped) {
            mPageDb.dirtyPage(node.mId);
        }
        /*P*/ // ]
        node.mContext.addDirty(node, mCommitState);
    }

    /**
     * Caller must hold commit lock and exclusive latch on node. Latch is always released by
     * this method, even if an exception is thrown.
     */
    void deleteNode(_Node node) throws IOException {
        deleteNode(node, true);
    }

    /**
     * Caller must hold commit lock and exclusive latch on node. Latch is always released by
     * this method, even if an exception is thrown.
     */
    void deleteNode(_Node node, boolean canRecycle) throws IOException {
        prepareToDelete(node);
        finishDeleteNode(node, canRecycle);
    }

    /**
     * Similar to markDirty method except no new page is reserved, and old page
     * is not immediately deleted. Caller must hold commit lock and exclusive
     * latch on node. Latch is never released by this method, unless an
     * exception is thrown.
     */
    void prepareToDelete(_Node node) throws IOException {
        // Hello. My name is Inigo Montoya. You killed my father. Prepare to die. 
        if (node.mCachedState == mCheckpointFlushState) {
            // _Node must be committed with the current checkpoint, and so
            // it must be written out before it can be deleted.
            try {
                node.write(mPageDb);
            } catch (Throwable e) {
                node.releaseExclusive();
                throw e;
            }
        }
    }

    /**
     * Caller must hold commit lock and exclusive latch on node. The
     * prepareToDelete method must have been called first. Latch is always
     * released by this method, even if an exception is thrown.
     */
    void finishDeleteNode(_Node node) throws IOException {
        finishDeleteNode(node, true);
    }

    /**
     * @param canRecycle true if node's page can be immediately re-used
     */
    void finishDeleteNode(_Node node, boolean canRecycle) throws IOException {
        try {
            long id = node.mId;

            if (id != 0) {
                // Must be removed from map before page is deleted. It could be recycled too
                // soon, creating a NodeMap collision.
                boolean removed = nodeMapRemove(node, Long.hashCode(id));

                try {
                    if (canRecycle && node.mCachedState == mCommitState) {
                        // Newly reserved page was never used, so recycle it.
                        mPageDb.recyclePage(id);
                    } else {
                        // Old data must survive until after checkpoint. Must force the delete,
                        // because by this point, the caller can't easily clean up.
                        mPageDb.deletePage(id, true);
                    }
                } catch (Throwable e) {
                    // Try to undo things.
                    if (removed) {
                        try {
                            nodeMapPut(node);
                        } catch (Throwable e2) {
                            Utils.suppress(e, e2);
                        }
                    }
                    throw e;
                }

                // When id is <= 1, it won't be moved to a secondary cache. Preserve the
                // original id for non-durable database to recycle it. Durable database relies
                // on the free list.
                node.mId = -id;
            }

            // When node is re-allocated, it will be evicted. Ensure that eviction
            // doesn't write anything.
            node.mCachedState = CACHED_CLEAN;
        } catch (Throwable e) {
            node.releaseExclusive();
            // Panic.
            close(e);
            throw e;
        }

        // Always releases the node latch.
        node.unused();
    }

    final byte[] fragmentKey(byte[] key) throws IOException {
        return fragment(key, key.length, mMaxKeySize);
    }

    final byte[] fragment(final byte[] value, final long vlength, int max)
        throws IOException
    {
        return fragment(value, vlength, max, 65535);
    }

    /**
     * Breakup a large value into separate pages, returning a new value which
     * encodes the page references. Caller must hold commit lock.
     *
     * Returned value begins with a one byte header:
     *
     * 0b0000_ffip
     *
     * The leading 4 bits define the encoding type, which must be 0. The 'f' bits define the
     * full value length field size: 2, 4, 6, or 8 bytes. The 'i' bit defines the inline
     * content length field size: 0 or 2 bytes. The 'p' bit is clear if direct pointers are
     * used, and set for indirect pointers. Pointers are always 6 bytes.
     *
     * @param value can be null if value is all zeros
     * @param max maximum allowed size for returned byte array; must not be
     * less than 11 (can be 9 if full value length is < 65536)
     * @param maxInline maximum allowed inline size; must not be more than 65535
     * @return null if max is too small
     */
    final byte[] fragment(final byte[] value, final long vlength, int max, int maxInline)
        throws IOException
    {
        final int pageSize = mPageSize;
        long pageCount = vlength / pageSize;
        final int remainder = (int) (vlength % pageSize);

        if (vlength >= 65536) {
            // Subtract header size, full length field size, and size of one pointer.
            max -= (1 + 4 + 6);
        } else if (pageCount == 0 && remainder <= (max - (1 + 2 + 2))) {
            // Entire value fits inline. It didn't really need to be
            // encoded this way, but do as we're told.
            byte[] newValue = new byte[(1 + 2 + 2) + (int) vlength];
            newValue[0] = 0x02; // ff=0, i=1, p=0
            encodeShortLE(newValue, 1, (int) vlength);     // full length
            encodeShortLE(newValue, 1 + 2, (int) vlength); // inline length
            arrayCopyOrFill(value, 0, newValue, (1 + 2 + 2), (int) vlength);
            return newValue;
        } else {
            // Subtract header size, full length field size, and size of one pointer.
            max -= (1 + 2 + 6);
        }

        if (max < 0) {
            return null;
        }

        long pointerSpace = pageCount * 6;

        byte[] newValue;
        final int inline; // length of inline field size
        if (remainder <= max && remainder <= maxInline
            && (pointerSpace <= (max + 6 - (inline = remainder == 0 ? 0 : 2) - remainder)))
        {
            // Remainder fits inline, minimizing internal fragmentation. All
            // extra pages will be full. All pointers fit too; encode direct.

            // Conveniently, 2 is the header bit and the inline length field size.
            byte header = (byte) inline;
            final int offset;
            if (vlength < (1L << (2 * 8))) {
                // (2 byte length field)
                offset = 1 + 2;
            } else if (vlength < (1L << (4 * 8))) {
                header |= 0x04; // ff = 1 (4 byte length field)
                offset = 1 + 4;
            } else if (vlength < (1L << (6 * 8))) {
                header |= 0x08; // ff = 2 (6 byte length field)
                offset = 1 + 6;
            } else {
                header |= 0x0c; // ff = 3 (8 byte length field)
                offset = 1 + 8;
            }

            int poffset = offset + inline + remainder;
            newValue = new byte[poffset + (int) pointerSpace];
            if (pageCount > 0) {
                if (value == null) {
                    // Value is sparse, so just fill with null pointers.
                    fill(newValue, poffset, poffset + ((int) pageCount) * 6, (byte) 0);
                } else {
                    try {
                        int voffset = remainder;
                        while (true) {
                            _Node node = allocDirtyFragmentNode();
                            try {
                                encodeInt48LE(newValue, poffset, node.mId);
                                p_copyFromArray(value, voffset, node.mPage, 0, pageSize);
                                if (pageCount == 1) {
                                    break;
                                }
                            } finally {
                                node.releaseExclusive();
                            }
                            pageCount--;
                            poffset += 6;
                            voffset += pageSize;
                        }
                    } catch (DatabaseException e) {
                        if (!e.isRecoverable()) {
                            close(e);
                        } else {
                            try {
                                // Clean up the mess.
                                while ((poffset -= 6) >= (offset + inline + remainder)) {
                                    deleteFragment(decodeUnsignedInt48LE(newValue, poffset));
                                }
                            } catch (Throwable e2) {
                                suppress(e, e2);
                                close(e);
                            }
                        }
                        throw e;
                    }
                }
            }

            newValue[0] = header;

            if (remainder != 0) {
                encodeShortLE(newValue, offset, remainder); // inline length
                arrayCopyOrFill(value, 0, newValue, offset + 2, remainder);
            }
        } else {
            // Remainder doesn't fit inline, so don't encode any inline
            // content. Last extra page will not be full.
            pageCount++;
            pointerSpace += 6;

            byte header;
            final int offset;
            if (vlength < (1L << (2 * 8))) {
                header = 0x00; // ff = 0, i=0
                offset = 1 + 2;
            } else if (vlength < (1L << (4 * 8))) {
                header = 0x04; // ff = 1, i=0
                offset = 1 + 4;
            } else if (vlength < (1L << (6 * 8))) {
                header = 0x08; // ff = 2, i=0
                offset = 1 + 6;
            } else {
                header = 0x0c; // ff = 3, i=0
                offset = 1 + 8;
            }

            if (pointerSpace <= (max + 6)) {
                // All pointers fit, so encode as direct.
                newValue = new byte[offset + (int) pointerSpace];
                if (pageCount > 0) {
                    if (value == null) {
                        // Value is sparse, so just fill with null pointers.
                        fill(newValue, offset, offset + ((int) pageCount) * 6, (byte) 0);
                    } else {
                        int poffset = offset;
                        try {
                            int voffset = 0;
                            while (true) {
                                _Node node = allocDirtyFragmentNode();
                                try {
                                    encodeInt48LE(newValue, poffset, node.mId);
                                    long page = node.mPage;
                                    if (pageCount > 1) {
                                        p_copyFromArray(value, voffset, page, 0, pageSize);
                                    } else {
                                        p_copyFromArray(value, voffset, page, 0, remainder);
                                        // Zero fill the rest, making it easier to extend later.
                                        p_clear(page, remainder, pageSize(page));
                                        break;
                                    }
                                } finally {
                                    node.releaseExclusive();
                                }
                                pageCount--;
                                poffset += 6;
                                voffset += pageSize;
                            }
                        } catch (DatabaseException e) {
                            if (!e.isRecoverable()) {
                                close(e);
                            } else {
                                try {
                                    // Clean up the mess.
                                    while ((poffset -= 6) >= offset) {
                                        deleteFragment(decodeUnsignedInt48LE(newValue, poffset));
                                    }
                                } catch (Throwable e2) {
                                    suppress(e, e2);
                                    close(e);
                                }
                            }
                            throw e;
                        }
                    }
                }
            } else {
                // Use indirect pointers.
                header |= 0x01;
                newValue = new byte[offset + 6];
                if (value == null) {
                    // Value is sparse, so just store a null pointer.
                    encodeInt48LE(newValue, offset, 0);
                } else {
                    int levels = calculateInodeLevels(vlength);
                    _Node inode = allocDirtyFragmentNode();
                    try {
                        encodeInt48LE(newValue, offset, inode.mId);
                        writeMultilevelFragments(levels, inode, value, 0, vlength);
                        inode.releaseExclusive();
                    } catch (DatabaseException e) {
                        if (!e.isRecoverable()) {
                            close(e);
                        } else {
                            try {
                                // Clean up the mess. Note that inode is still latched here,
                                // because writeMultilevelFragments never releases it. The call to
                                // deleteMultilevelFragments always releases the inode latch.
                                deleteMultilevelFragments(levels, inode, vlength);
                            } catch (Throwable e2) {
                                suppress(e, e2);
                                close(e);
                            }
                        }
                        throw e;
                    } catch (Throwable e) {
                        close(e);
                        throw e;
                    }
                }
            }

            newValue[0] = header;
        }

        // Encode full length field.
        if (vlength < (1L << (2 * 8))) {
            encodeShortLE(newValue, 1, (int) vlength);
        } else if (vlength < (1L << (4 * 8))) {
            encodeIntLE(newValue, 1, (int) vlength);
        } else if (vlength < (1L << (6 * 8))) {
            encodeInt48LE(newValue, 1, vlength);
        } else {
            encodeLongLE(newValue, 1, vlength);
        }

        return newValue;
    }

    int calculateInodeLevels(long vlength) {
        long[] caps = mFragmentInodeLevelCaps;
        int levels = 0;
        while (levels < caps.length) {
            if (vlength <= caps[levels]) {
                break;
            }
            levels++;
        }
        return levels;
    }

    static long decodeFullFragmentedValueLength(int header, long fragmented, int off) {
        switch ((header >> 2) & 0x03) {
        default:
            return p_ushortGetLE(fragmented, off);
        case 1:
            return p_intGetLE(fragmented, off) & 0xffffffffL;
        case 2:
            return p_uint48GetLE(fragmented, off);
        case 3:
            return p_longGetLE(fragmented, off);
        }
    }

    /**
     * @param level inode level; at least 1
     * @param inode exclusive latched parent inode; never released by this method
     * @param value slice of complete value being fragmented
     */
    private void writeMultilevelFragments(int level, _Node inode,
                                          byte[] value, int voffset, long vlength)
        throws IOException
    {
        long page = inode.mPage;
        level--;
        long levelCap = levelCap(level);

        int childNodeCount = childNodeCount(vlength, levelCap);

        int poffset = 0;
        try {
            for (int i=0; i<childNodeCount; i++) {
                _Node childNode = allocDirtyFragmentNode();
                p_int48PutLE(page, poffset, childNode.mId);
                poffset += 6;

                int len = (int) Math.min(levelCap, vlength);
                if (level <= 0) {
                    long childPage = childNode.mPage;
                    p_copyFromArray(value, voffset, childPage, 0, len);
                    // Zero fill the rest, making it easier to extend later.
                    p_clear(childPage, len, pageSize(childPage));
                    childNode.releaseExclusive();
                } else {
                    try {
                        writeMultilevelFragments(level, childNode, value, voffset, len);
                    } finally {
                        childNode.releaseExclusive();
                    }
                }

                vlength -= len;
                voffset += len;
            }
        } finally {
            // Zero fill the rest, making it easier to extend later. If an exception was
            // thrown, this simplies cleanup. All of the allocated pages are referenced,
            // but the rest are not.
            p_clear(page, poffset, pageSize(page));
        }
    }

    /**
     * Determine the multi-level fragmented value child node count, at a specific level.
     */
    private static int childNodeCount(long vlength, long levelCap) {
        int count = (int) ((vlength + (levelCap - 1)) / levelCap);
        if (count < 0) {
            // Overflowed.
            count = childNodeCountOverflow(vlength, levelCap);
        }
        return count;
    }

    private static int childNodeCountOverflow(long vlength, long levelCap) {
        return BigInteger.valueOf(vlength).add(BigInteger.valueOf(levelCap - 1))
            .divide(BigInteger.valueOf(levelCap)).intValue();
    }

    /**
     * Reconstruct a fragmented key.
     */
    byte[] reconstructKey(long fragmented, int off, int len) throws IOException {
        try {
            return reconstruct(fragmented, off, len);
        } catch (LargeValueException e) {
            throw new LargeKeyException(e.getLength(), e.getCause());
        }
    }

    /**
     * Reconstruct a fragmented value.
     */
    byte[] reconstruct(long fragmented, int off, int len) throws IOException {
        return reconstruct(fragmented, off, len, null);
    }

    /**
     * Reconstruct a fragmented value.
     *
     * @param stats non-null for stats: [0]: full length, [1]: number of pages (>0 if fragmented)
     * @return null if stats requested
     */
    byte[] reconstruct(long fragmented, int off, int len, long[] stats)
        throws IOException
    {
        int header = p_byteGet(fragmented, off++);
        len--;

        long vLen;
        switch ((header >> 2) & 0x03) {
        default:
            vLen = p_ushortGetLE(fragmented, off);
            break;

        case 1:
            vLen = p_intGetLE(fragmented, off);
            if (vLen < 0) {
                vLen &= 0xffffffffL;
                if (stats == null) {
                    throw new LargeValueException(vLen);
                }
            }
            break;

        case 2:
            vLen = p_uint48GetLE(fragmented, off);
            if (vLen > Integer.MAX_VALUE && stats == null) {
                throw new LargeValueException(vLen);
            }
            break;

        case 3:
            vLen = p_longGetLE(fragmented, off);
            if (vLen < 0 || (vLen > Integer.MAX_VALUE && stats == null)) {
                throw new LargeValueException(vLen);
            }
            break;
        }

        {
            int vLenFieldSize = 2 + ((header >> 1) & 0x06);
            off += vLenFieldSize;
            len -= vLenFieldSize;
        }

        byte[] value;
        if (stats != null) {
            stats[0] = vLen;
            value = null;
        } else {
            try {
                value = new byte[(int) vLen];
            } catch (OutOfMemoryError e) {
                throw new LargeValueException(vLen, e);
            }
        }

        int vOff = 0;
        if ((header & 0x02) != 0) {
            // Inline content.
            int inLen = p_ushortGetLE(fragmented, off);
            off += 2;
            len -= 2;
            if (value != null) {
                p_copyToArray(fragmented, off, value, vOff, inLen);
            }
            off += inLen;
            len -= inLen;
            vOff += inLen;
            vLen -= inLen;
        }

        long pagesRead = 0;

        if ((header & 0x01) == 0) {
            // Direct pointers.
            while (len >= 6) {
                long nodeId = p_uint48GetLE(fragmented, off);
                off += 6;
                len -= 6;
                int pLen;
                if (nodeId == 0) {
                    // Reconstructing a sparse value. Array is already zero-filled.
                    pLen = Math.min((int) vLen, mPageSize);
                } else {
                    _Node node = nodeMapLoadFragment(nodeId);
                    pagesRead++;
                    try {
                        long page = node.mPage;
                        pLen = Math.min((int) vLen, pageSize(page));
                        if (value != null) {
                            p_copyToArray(page, 0, value, vOff, pLen);
                        }
                    } finally {
                        node.releaseShared();
                    }
                }
                vOff += pLen;
                vLen -= pLen;
            }
        } else {
            // Indirect pointers.
            long inodeId = p_uint48GetLE(fragmented, off);
            if (inodeId != 0) {
                _Node inode = nodeMapLoadFragment(inodeId);
                pagesRead++;
                int levels = calculateInodeLevels(vLen);
                pagesRead += readMultilevelFragments(levels, inode, value, vOff, vLen);
            }
        }

        if (stats != null) {
            stats[1] = pagesRead;
        }

        return value;
    }

    /**
     * @param level inode level; at least 1
     * @param inode shared latched parent inode; always released by this method
     * @param value slice of complete value being reconstructed; initially filled with zeros;
     * pass null for stats only
     * @return number of pages read
     */
    private long readMultilevelFragments(int level, _Node inode,
                                         byte[] value, int voffset, long vlength)
        throws IOException
    {
        try {
            long pagesRead = 0;

            long page = inode.mPage;
            level--;
            long levelCap = levelCap(level);

            int childNodeCount = childNodeCount(vlength, levelCap);

            for (int poffset = 0, i=0; i<childNodeCount; poffset += 6, i++) {
                long childNodeId = p_uint48GetLE(page, poffset);
                int len = (int) Math.min(levelCap, vlength);

                if (childNodeId != 0) {
                    _Node childNode = nodeMapLoadFragment(childNodeId);
                    pagesRead++;
                    if (level <= 0) {
                        if (value != null) {
                            p_copyToArray(childNode.mPage, 0, value, voffset, len);
                        }
                        childNode.releaseShared();
                    } else {
                        pagesRead += readMultilevelFragments
                            (level, childNode, value, voffset, len);
                    }
                }

                vlength -= len;
                voffset += len;
            }

            return pagesRead;
        } finally {
            inode.releaseShared();
        }
    }

    /**
     * Delete the extra pages of a fragmented value. Caller must hold commit lock.
     *
     * @param fragmented page containing fragmented value 
     */
    void deleteFragments(long fragmented, int off, int len)
        throws IOException
    {
        int header = p_byteGet(fragmented, off++);
        len--;

        long vLen;
        if ((header & 0x01) == 0) {
            // Don't need to read the value length when deleting direct pointers.
            vLen = 0;
        } else {
            switch ((header >> 2) & 0x03) {
            default:
                vLen = p_ushortGetLE(fragmented, off);
                break;
            case 1:
                vLen = p_intGetLE(fragmented, off) & 0xffffffffL;
                break;
            case 2:
                vLen = p_uint48GetLE(fragmented, off);
                break;
            case 3:
                vLen = p_longGetLE(fragmented, off);
                break;
            }
        }

        {
            int vLenFieldSize = 2 + ((header >> 1) & 0x06);
            off += vLenFieldSize;
            len -= vLenFieldSize;
        }

        if ((header & 0x02) != 0) {
            // Skip inline content.
            int inLen = 2 + p_ushortGetLE(fragmented, off);
            off += inLen;
            len -= inLen;
        }

        if ((header & 0x01) == 0) {
            // Direct pointers.
            while (len >= 6) {
                long nodeId = p_uint48GetLE(fragmented, off);
                off += 6;
                len -= 6;
                deleteFragment(nodeId);
            }
        } else {
            // Indirect pointers.
            long inodeId = p_uint48GetLE(fragmented, off);
            if (inodeId != 0) {
                _Node inode = removeInode(inodeId);
                int levels = calculateInodeLevels(vLen);
                deleteMultilevelFragments(levels, inode, vLen);
            }
        }
    }

    /**
     * @param level inode level; at least 1
     * @param inode exclusive latched parent inode; always released by this method
     */
    private void deleteMultilevelFragments(int level, _Node inode, long vlength)
        throws IOException
    {
        long page = inode.mPage;
        level--;
        long levelCap = levelCap(level);

        // Copy all child node ids and release parent latch early.
        int childNodeCount = childNodeCount(vlength, levelCap);
        long[] childNodeIds = new long[childNodeCount];
        for (int poffset = 0, i=0; i<childNodeCount; poffset += 6, i++) {
            childNodeIds[i] = p_uint48GetLE(page, poffset);
        }
        deleteNode(inode);

        if (level <= 0) for (long childNodeId : childNodeIds) {
            deleteFragment(childNodeId);
        } else for (long childNodeId : childNodeIds) {
            long len = Math.min(levelCap, vlength);
            if (childNodeId != 0) {
                _Node childNode = removeInode(childNodeId);
                deleteMultilevelFragments(level, childNode, len);
            }
            vlength -= len;
        }
    }

    /**
     * @param nodeId must not be zero
     * @return non-null _Node with exclusive latch held
     */
    private _Node removeInode(long nodeId) throws IOException {
        _Node node = nodeMapGetAndRemove(nodeId);
        if (node == null) {
            node = allocLatchedNode(nodeId, _NodeContext.MODE_UNEVICTABLE);
            /*P*/ // [
            // node.type(TYPE_FRAGMENT);
            /*P*/ // ]
            readNode(node, nodeId);
        }
        return node;
    }

    /**
     * @param nodeId can be zero
     */
    void deleteFragment(long nodeId) throws IOException {
        if (nodeId != 0) {
            _Node node = nodeMapGetAndRemove(nodeId);
            if (node != null) {
                deleteNode(node);
            } else try {
                if (mInitialReadState != CACHED_CLEAN) {
                    // Page was never used if nothing has ever been checkpointed.
                    mPageDb.recyclePage(nodeId);
                } else {
                    // Page is clean if not in a _Node, and so it must survive until after the
                    // next checkpoint. Must force the delete, because by this point, the
                    // caller can't easily clean up.
                    mPageDb.deletePage(nodeId, true);
                }
            } catch (Throwable e) {
                // Panic.
                close(e);
                throw e;
            }
        }
    }

    private static long[] calculateInodeLevelCaps(int pageSize) {
        long[] caps = new long[10];
        long cap = pageSize;
        long scalar = pageSize / 6; // 6-byte pointers

        int i = 0;
        while (i < caps.length) {
            caps[i++] = cap;
            long next = cap * scalar;
            if (next / scalar != cap) {
                caps[i++] = Long.MAX_VALUE;
                break;
            }
            cap = next;
        }

        if (i < caps.length) {
            long[] newCaps = new long[i];
            arraycopy(caps, 0, newCaps, 0, i);
            caps = newCaps;
        }

        return caps;
    }

    long levelCap(int level) {
        return mFragmentInodeLevelCaps[level];
    }

    /**
     * If fragmented trash exists, non-transactionally delete all fragmented values. Expected
     * to be called only during recovery or replication leader switch.
     */
    private void emptyAllFragmentedTrash(boolean checkpoint) throws IOException {
        _FragmentedTrash trash = mFragmentedTrash;
        if (trash != null && trash.emptyAllTrash(mEventListener) && checkpoint) {
            checkpoint(false, 0, 0);
        }
    }

    /**
     * Obtain the trash for transactionally deleting fragmented values.
     */
    _FragmentedTrash fragmentedTrash() throws IOException {
        _FragmentedTrash trash = mFragmentedTrash;
        if (trash != null) {
            return trash;
        }
        mOpenTreesLatch.acquireExclusive();
        try {
            if ((trash = mFragmentedTrash) != null) {
                return trash;
            }
            _Tree tree = openInternalTree(_Tree.FRAGMENTED_TRASH_ID, true);
            return mFragmentedTrash = new _FragmentedTrash(tree);
        } finally {
            mOpenTreesLatch.releaseExclusive();
        }
    }

    long removeSparePage() {
        return mSparePagePool.remove();
    }

    void addSparePage(long page) {
        mSparePagePool.add(page);
    }

    /**
     * Reads the node page, sets the id and cached state. _Node must be latched exclusively.
     */
    void readNode(_Node node, long id) throws IOException {
        /*P*/ // [
        // mPageDb.readPage(id, node.mPage);
        /*P*/ // |
        if (mFullyMapped) {
            node.mPage = mPageDb.directPagePointer(id);
        } else {
            mPageDb.readPage(id, node.mPage);
        }
        /*P*/ // ]

        node.mId = id;

        // NOTE: If initial state is clean, an optimization is possible, but it's a bit
        // tricky. Too many pages are allocated when evictions are high, write rate is high,
        // and commits are bogged down.  Keep some sort of cache of ids known to be dirty. If
        // reloaded before commit, then they're still dirty.
        //
        // A Bloom filter is not appropriate, because of false positives. A random evicting
        // cache works well -- it has no collision chains. Evict whatever else was there in
        // the slot. An array of longs should suffice.
        //
        // When a child node is loaded with a dirty state, the parent nodes must be updated
        // as well. This might force them to be evicted, and then the optimization is
        // lost. A better approach would avoid the optimization if the parent node is clean
        // or doesn't match the current commit state.

        node.mCachedState = mInitialReadState;
    }

    @Override
    EventListener eventListener() {
        return mEventListener;
    }

    @Override
    void checkpoint(boolean force, long sizeThreshold, long delayThresholdNanos)
        throws IOException
    {
        // Checkpoint lock ensures consistent state between page store and logs.
        mCheckpointLock.lock();
        try {
            if (isClosed()) {
                return;
            }

            // Now's a good time to clean things up.
            cleanupUnreferencedTrees();

            final _Node root = mRegistry.mRoot;

            long nowNanos = System.nanoTime();

            if (!force) {
                thresholdCheck : {
                    if (delayThresholdNanos == 0) {
                        break thresholdCheck;
                    }

                    if (delayThresholdNanos > 0 &&
                        ((nowNanos - mLastCheckpointNanos) >= delayThresholdNanos))
                    {
                        break thresholdCheck;
                    }

                    if (mRedoWriter == null || mRedoWriter.shouldCheckpoint(sizeThreshold)) {
                        break thresholdCheck;
                    }

                    // Thresholds not met for a full checkpoint, but fully sync the redo log
                    // for durability.
                    flush(2); // flush and sync metadata

                    return;
                }

                // Thresholds for a full checkpoint are met.
                treeCheck: {
                    root.acquireShared();
                    try {
                        if (root.mCachedState != CACHED_CLEAN) {
                            // Root is dirty, do a full checkpoint.
                            break treeCheck;
                        }
                    } finally {
                        root.releaseShared();
                    }

                    // Root is clean, so no need for full checkpoint, but fully sync the redo
                    // log for durability.
                    flush(2); // flush and sync metadata

                    return;
                }
            }

            mLastCheckpointNanos = nowNanos;

            if (mEventListener != null) {
                // Note: Events should not be delivered when exclusive commit lock is held.
                // The listener implementation might introduce extra blocking.
                mEventListener.notify(EventType.CHECKPOINT_BEGIN, "Checkpoint begin");
            }

            boolean resume = true;

            long header = mCommitHeader;
            _UndoLog masterUndoLog = mCommitMasterUndoLog;

            if (header == p_null()) {
                // Not resumed. Allocate new header early, before acquiring locks.
                header = p_calloc(mPageDb.pageSize(), mPageDb.isDirectIO());
                resume = false;
                if (masterUndoLog != null) {
                    // TODO: Thrown when closed? After storage device was full.
                    throw new AssertionError();
                }
            }

            final _RedoWriter redo = mRedoWriter;

            try {
                int hoff = mPageDb.extraCommitDataOffset();
                p_intPutLE(header, hoff + I_ENCODING_VERSION, ENCODING_VERSION);

                if (redo != null) {
                    // File-based redo log should create a new file, but not write to it yet.
                    redo.checkpointPrepare();
                }

                while (true) {
                    mCommitLock.acquireExclusive();

                    // Registry root is infrequently modified, and so shared latch
                    // is usually available. If not, cause might be a deadlock. To
                    // be safe, always release commit lock and start over.
                    if (root.tryAcquireShared()) {
                        break;
                    }

                    mCommitLock.releaseExclusive();
                }

                mCheckpointFlushState = CHECKPOINT_FLUSH_PREPARE;

                if (!resume) {
                    p_longPutLE(header, hoff + I_ROOT_PAGE_ID, root.mId);
                }

                final long redoNum, redoPos, redoTxnId;
                if (redo == null) {
                    redoNum = 0;
                    redoPos = 0;
                    redoTxnId = 0;
                } else {
                    // Switch and capture state while commit lock is held.
                    redo.checkpointSwitch(mTxnContexts);
                    redoNum = redo.checkpointNumber();
                    redoPos = redo.checkpointPosition();
                    redoTxnId = redo.checkpointTransactionId();
                }

                p_longPutLE(header, hoff + I_CHECKPOINT_NUMBER, redoNum);
                p_longPutLE(header, hoff + I_REDO_TXN_ID, redoTxnId);
                p_longPutLE(header, hoff + I_REDO_POSITION, redoPos);
                p_longPutLE(header, hoff + I_REPL_ENCODING, redo == null ? 0 : redo.encoding());

                // TODO: I don't like all this activity with exclusive commit
                // lock held. _UndoLog can be refactored to store into a special
                // _Tree, but this requires more features to be added to _Tree
                // first. Specifically, large values and appending to them.

                long txnId = 0;
                final long masterUndoLogId;

                if (resume) {
                    masterUndoLogId = masterUndoLog == null ? 0 : masterUndoLog.topNodeId();
                } else {
                    byte[] workspace = null;

                    for (_TransactionContext txnContext : mTxnContexts) {
                        txnId = txnContext.higherTransactionId(txnId);

                        synchronized (txnContext) {
                            if (txnContext.hasUndoLogs()) {
                                if (masterUndoLog == null) {
                                    masterUndoLog = new _UndoLog(this, 0);
                                }
                                workspace = txnContext.writeToMaster(masterUndoLog, workspace);
                            }
                        }
                    }

                    if (masterUndoLog == null) {
                        masterUndoLogId = 0;
                    } else {
                        masterUndoLogId = masterUndoLog.persistReady();
                        if (masterUndoLogId == 0) {
                            // Nothing was actually written to the log.
                            masterUndoLog = null;
                        }
                    }

                    // Stash it to resume after an aborted checkpoint.
                    mCommitMasterUndoLog = masterUndoLog;
                }

                p_longPutLE(header, hoff + I_TRANSACTION_ID, txnId);
                p_longPutLE(header, hoff + I_MASTER_UNDO_LOG_PAGE_ID, masterUndoLogId);

                mPageDb.commit(resume, header, (boolean resume_, long header_) -> {
                    flush(resume_, header_);
                });
            } catch (Throwable e) {
                if (mCommitHeader != header) {
                    p_delete(header);
                }

                if (mCheckpointFlushState == CHECKPOINT_FLUSH_PREPARE) {
                    // Exception was thrown with locks still held.
                    mCheckpointFlushState = CHECKPOINT_NOT_FLUSHING;
                    root.releaseShared();
                    mCommitLock.releaseExclusive();
                    if (redo != null) {
                        redo.checkpointAborted();
                    }
                }

                throw e;
            }

            // Reset for next checkpoint.
            deleteCommitHeader();
            mCommitMasterUndoLog = null;

            if (masterUndoLog != null) {
                // Delete the master undo log, which won't take effect until
                // the next checkpoint.
                CommitLock.Shared shared = mCommitLock.acquireShared();
                try {
                    if (!isClosed()) {
                        shared = masterUndoLog.doTruncate(mCommitLock, shared, false);
                    }
                } finally {
                    shared.release();
                }
            }

            // Note: This step is intended to discard old redo data, but it can
            // get skipped if process exits at this point. Data is discarded
            // again when database is re-opened.
            if (mRedoWriter != null) {
                mRedoWriter.checkpointFinished();
            }

            if (mEventListener != null) {
                double duration = (System.nanoTime() - mLastCheckpointNanos) / 1_000_000_000.0;
                mEventListener.notify(EventType.CHECKPOINT_COMPLETE,
                                      "Checkpoint completed in %1$1.3f seconds",
                                      duration, TimeUnit.SECONDS);
            }
        } finally {
            mCheckpointLock.unlock();
        }
    }

    /**
     * Method is invoked with exclusive commit lock and shared root node latch
     * held. Both are released by this method.
     */
    private void flush(final boolean resume, final long header) throws IOException {
        Object custom = mCustomTxnHandler;
        if (custom != null) {
            custom = mCustomTxnHandler.checkpointStart(this);
        }

        int stateToFlush = mCommitState;

        if (resume) {
            // Resume after an aborted checkpoint.
            if (header != mCommitHeader) {
                throw new AssertionError();
            }
            stateToFlush ^= 1;
        } else {
            if (mInitialReadState != CACHED_CLEAN) {
                mInitialReadState = CACHED_CLEAN; // Must be set before switching commit state.
            }
            mCommitState = (byte) (stateToFlush ^ 1);
            mCommitHeader = header;
        }

        mCheckpointFlushState = stateToFlush;

        mRegistry.mRoot.releaseShared();
        mCommitLock.releaseExclusive();

        if (mRedoWriter != null) {
            mRedoWriter.checkpointStarted();
        }

        if (mEventListener != null) {
            mEventListener.notify(EventType.CHECKPOINT_FLUSH, "Flushing all dirty nodes");
        }

        try {
            for (_NodeContext context : mNodeContexts) {
                context.flushDirty(stateToFlush);
            }

            if (mRedoWriter != null) {
                mRedoWriter.checkpointFlushed();
            }

            if (mCustomTxnHandler != null) {
                mCustomTxnHandler.checkpointFinish(this, custom);
            }
        } finally {
            mCheckpointFlushState = CHECKPOINT_NOT_FLUSHING;
        }

        if (mEventListener != null) {
            mEventListener.notify(EventType.CHECKPOINT_SYNC, "Forcibly persisting all changes");
        }
    }

    // Called by _DurablePageDb with header latch held.
    static long readRedoPosition(long header, int offset) {
        return p_longGetLE(header, offset + I_REDO_POSITION);
    }
}
